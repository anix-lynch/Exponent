question_id,question_text,priority,pattern_id,pattern_name,solving_formula,notes,short_answer
2884,"Your largest customer is advocating for a new feature not in your roadmap. What do you do?",ðŸŸ¢,P6,Prioritization,"Impact Ã— Confidence Ã— Ease â†’ RICE Score â†’ Decide + Communicate","Customer advocacy = high signal but check broader impact","I'd evaluate this using RICE: Impact (how many users benefit?), Confidence (do we have data supporting this?), Ease (engineering cost). Compare this against current roadmap items. If it scores high, consider swapping or adding capacity. Communicate the decision back to the customer with clear reasoningâ€”either 'yes, here's the timeline' or 'not now, here's why and what we're prioritizing instead.' Key is transparency and data-driven trade-offs."
2885,"Your recently launched feature isn't meeting customer expectations, but your engineering team and roadmap are at capacity. How would you address this?",ðŸŸ¢,P6,Prioritization,"Assess Gap â†’ Quick Wins vs Deep Fix â†’ Resource Trade-off â†’ Decide","Capacity constraint = force rank ruthlessly","First, I'd assess the gapâ€”what specifically isn't meeting expectations? Then categorize fixes: quick wins (small tweaks, high impact) vs deep fixes (re-architecture). Given capacity constraints, I'd prioritize quick wins immediately and negotiate roadmap trade-offs for deep fixes. Present options to leadership: delay lower-priority work, add capacity, or accept the gap. Make the trade-off explicit and get alignment."
2886,"YouTube's average video buffering time has increased by 10%. How would you address this issue?",ðŸŸ¢,P1,Metric Drop Diagnosis,"Clarify Metric â†’ Segment (geo/device/content) â†’ Hypothesize â†’ Data Check â†’ Action","Latency metric; likely infra or CDN issue","First, clarify the metricâ€”average across all videos or specific segments? Then segment by geography, device type, and content type to isolate where the 10% increase is concentrated. Hypothesize causes: CDN issues, increased traffic, video encoding changes, or infrastructure bottlenecks. Validate with dataâ€”server logs, CDN metrics, user complaints. Finally, propose fixes based on root cause: scale infrastructure, optimize encoding, or adjust CDN routing."
2887,"You're a PM at CarGurus. You launched in Canada 8 months ago and conversions are lower in Canada than the US. What would you do?",ðŸŸ¢,P1,Metric Drop Diagnosis,"Clarify Metric â†’ Segment (Canada vs US) â†’ Hypothesize (product-market fit) â†’ Validate â†’ Fix","Geographic comparison = localization or market fit","First, clarify the conversion metricâ€”listing views to leads, or leads to sales? Then compare Canada vs US across key dimensions: user behavior, inventory quality, pricing, and localization. Hypothesize causes: product-market fit issues (Canadian users have different needs), inventory gaps (fewer Canadian listings), or UX friction (currency, language). Validate with user research and data analysis. Fix based on findingsâ€”improve inventory, adjust UX, or refine go-to-market strategy."
2888,"You're asked to identify high-value customersâ€”how would you approach this using data?",ðŸŸ¢,P5,Segmentation,"(Persona Ã— Behavior Ã— Value) â†’ Rank â†’ Focus Top Segments","Value = LTV or revenue proxy","I'd segment customers across three dimensions: persona (demographics, firmographics), behavior (usage frequency, feature adoption, engagement), and value (LTV, revenue, retention). Then rank segments by strategic fitâ€”high value + high engagement signals where we can drive the most impact. I'd focus resources on the top 2-3 segments and build targeted strategies for acquisition, retention, and expansion within those segments."
2889,"You're given product adoption data across regionsâ€”how would you determine where the rollout was most successful and why?",ðŸŸ¢,P5,Segmentation,"Define Success Metric â†’ Segment by Region â†’ Compare â†’ Root Cause (why) â†’ Replicate","Regional comparison = look for adoption patterns","First, define successâ€”adoption rate, engagement, retention, or revenue? Then segment by region and compare performance. Identify top-performing regions and analyze why: better product-market fit, stronger go-to-market execution, favorable market conditions, or different user behavior. Look for patternsâ€”demographics, competitive landscape, distribution channels. Finally, extract learnings and replicate successful tactics in underperforming regions."
2890,"You're launching a new featureâ€”how would you measure its strategic impact on growth?",ðŸŸ¢,P2,NSM + KPI Ladder,"Define NSM â†’ Input KPIs â†’ Leading Indicators â†’ Guardrails â†’ Dashboard","Growth impact = tie to business goal clearly","I'd start by defining the North Star Metric this feature should moveâ€”e.g., active users, revenue, or retention. Then build a KPI ladder: input metrics (feature adoption, usage frequency), leading indicators (early engagement signals), and guardrails (quality metrics like error rates, satisfaction). Create a dashboard to track these over time. Tie the feature's performance directly to the business goalâ€”if NSM moves, the feature is working; if not, diagnose and iterate."
2891,"You're told engagement is flat even though new users are increasingâ€”what questions would you ask and what data would you look at?",ðŸŸ¢,P1,Metric Drop Diagnosis,"Clarify Engagement â†’ Cohort Analysis (new vs old) â†’ Activation vs Retention â†’ Diagnose","Flat engagement + new users = activation or dilution issue","First, clarify engagementâ€”DAU, session length, or feature usage? Then run cohort analysis: are new users less engaged than old users, or is overall engagement declining? Check activation (are new users reaching 'aha' moments?) and retention (are old users churning?). Hypothesize: new user quality dropped, onboarding is broken, or product is diluting. Validate with dataâ€”activation funnels, retention curves, user feedback. Fix based on root cause."
2892,"You've been asked to analyze the performance of a newly launched onboarding experience. What steps would you take?",ðŸŸ¢,P3,Funnel Analysis,"Define Funnel Steps â†’ Measure Drop-off â†’ Identify Friction â†’ Hypothesize Fix â†’ Test","Onboarding = activation funnel; focus on biggest drop","First, define the onboarding funnel stepsâ€”sign-up, profile setup, first action, activation milestone. Measure drop-off rates at each step. Identify the biggest friction pointsâ€”where are users abandoning? Hypothesize causes: unclear value prop, too many steps, technical issues, or poor UX. Validate with user research, session recordings, and data analysis. Propose fixesâ€”simplify steps, improve messaging, reduce friction. Test changes with A/B experiments and iterate."
2893,"You've joined a program that has gone through many changes. How would you restructure it to get it back on track?",ðŸŸ¢,P12,Operational Excellence,"Assess Current State â†’ Identify Risks â†’ Prioritize Fixes â†’ Communicate Plan â†’ Monitor","Program recovery = stabilize then optimize","First, assess the current stateâ€”what's broken, what's working, and why the changes caused instability. Identify top risksâ€”missed deadlines, team morale, unclear goals, or technical debt. Prioritize fixes: stabilize critical paths first, then optimize. Communicate a clear plan to stakeholdersâ€”what we're fixing, why, and the timeline. Set up monitoringâ€”regular check-ins, dashboards, and escalation paths. Focus on quick wins to rebuild momentum and trust."
