question_id,question_text,priority,pattern_id,pattern_name,solving_formula,notes,short_answer
183,Debug a metric that was off by x percentage.,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Metric debugging; data quality,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
248,Describe your experience with statistics.,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Statistics experience; technical background,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
790,Explain data drifting.,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data quality explanation; ML monitoring,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
803,Explain how you handle data scarcity.,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data quality; ML problem,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
915,"Given a feature data record (FDR), how would you detect anomalies in it?",ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Anomaly detection; data quality,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
1006,Have you ever had to work with poor-quality data or suggest new tracking?,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data quality; behavioral question,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
1007,Have you ever made a decision based on bad data? What did you learn?,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data decision; behavioral question,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
1061,How do you clean and prepare troublesome datasets?,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data cleaning; data quality,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
1093,How do you ensure data governance policies are applied consistently across different data sources?,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data governance; data quality,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
1108,How do you handle joining data from different sources with inconsistent IDs?,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data integration; data quality,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
1114,How do you handle working with ambiguous data?,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Ambiguous data; data quality,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
1240,How would you address the issue of unstructured data in the data warehouse to improve report efficiency for clients?,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data quality; data warehouse,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
1248,How would you approach data curation for an LLM training pipeline?,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data curation; data quality,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
1273,How would you check for bias in your data?,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data bias; data quality,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
1871,Tell me about a project where you faced data issues and how you handled them.,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data issues; behavioral question,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
1872,Tell me about a project where you had to clean and organize a large dataset.,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data cleaning; behavioral question,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
2095,Tell me about a time you worked on a data-intensive project.,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data project; behavioral question,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
2180,Walk me through a past data science project.,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data science project; behavioral question,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
2186,Walk me through your process for cleaning a messy dataset.,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data cleaning; behavioral question,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
2215,What are the implications of collecting excessive user data or too many survey responses?,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data collection; data quality,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
2257,"What data tools have you worked with, and what specific projects did you use those tools for?",ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data tools; behavioral question,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
2263,What do you know about data annotation and collecting data for LLM training?,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data annotation; data quality,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
2325,What is Pyspark?,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data tool; data quality,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
2471,What would you do if you didn't have access to the exact data you need?,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data access; data quality,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
2517,"What's the most complex SQL query you've written? Walk me through what it did, the key concepts involved, and the types of joins you used.",ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,SQL query; behavioral question,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
2523,When is Hadoop better than PySpark?,ðŸŸ¡,L1,Data Trust,Source â†’ Freshness â†’ Completeness â†’ Bias â†’ Sanity Checks,Data tool comparison; data quality,"Source: Identify origin and validate ownership. Identify origin: Primary: product logs, first-party events, direct user actions. Secondary: internal pipelines, transformations, aggregated data. External: vendors, partners, scraped data, third-party APIs. Validate ownership: Who maintains it? Who is on-call when it breaks? Is there documentation / lineage? Can we trace the data flow?

Freshness: Check if data is up to date. Expected latency: Real-time: should update immediately. Hourly: should update within the hour. Daily / Batch: should update on schedule. Check gaps: Last updated timestamp: when was it last refreshed? Delays vs SLA: is it meeting expected update frequency? Silent failures: no alerts but stale data (check for missing updates).

Completeness: Verify nothing is missing. Coverage checks: Missing rows / days: are there gaps in the time series? Null or default-heavy fields: are key fields populated? Partial segments dropped: are all user segments / platforms included? Join loss: Inner joins removing data: are we losing records in joins? Key mismatches: do join keys align correctly? Upstream schema changes: did schema changes break data collection?

Bias: Check who is over- or under-represented. Sampling bias: Logged-in only: are we missing anonymous users? Power users: are we over-representing heavy users? Specific regions / platforms: are all geos and devices included? Measurement bias: Proxy â‰  true behavior: is our metric a good proxy for what we care about? Instrumentation gaps: are we tracking all relevant events? Incentives to game metrics: could users or teams be gaming the measurement?

Sanity Checks: Verify numbers pass the smell test. Trend checks: sharp jumps/drops that don't align with known changes. Ratio checks: conversion > 100%? Negative values where impossible? Cross-metric consistency: do related metrics move together logically? Compare to historical baselines: is this within expected range? Remember: No decision is better than a confident decision on bad data. If trust is low â†’ slow down, qualify, or re-measure."
