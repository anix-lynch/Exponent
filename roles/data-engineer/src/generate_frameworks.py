"""
Generate Data Engineer Question Bank with frameworks for each category
"""
import json
import os

# Load categorized questions
with open('data/questions_by_category.json', 'r') as f:
    questions_by_category = json.load(f)

# DE-specific frameworks for each category
FRAMEWORKS = {
    "Data Pipeline Design": """
Data Pipeline Design
â”œâ”€ Understand requirements
â”‚  â”œâ”€ Data sources (where?)
â”‚  â”œâ”€ Data volume (how much?)
â”‚  â”œâ”€ Data velocity (how fast?)
â”‚  â”œâ”€ Data variety (what types?)
â”‚  â””â”€ SLAs (how often?)
â”‚
â”œâ”€ Design ingestion
â”‚  â”œâ”€ Batch vs streaming
â”‚  â”œâ”€ Pull vs push
â”‚  â”œâ”€ Full vs incremental
â”‚  â””â”€ Data formats (JSON, CSV, Parquet)
â”‚
â”œâ”€ Plan transformation
â”‚  â”œâ”€ Cleaning (nulls, duplicates)
â”‚  â”œâ”€ Enrichment (joins, lookups)
â”‚  â”œâ”€ Aggregation (group by, window)
â”‚  â””â”€ Business logic
â”‚
â”œâ”€ Choose storage
â”‚  â”œâ”€ Data warehouse (Snowflake, Redshift)
â”‚  â”œâ”€ Data lake (S3, ADLS)
â”‚  â”œâ”€ Database (PostgreSQL, MongoDB)
â”‚  â””â”€ Cache (Redis, Memcached)
â”‚
â”œâ”€ Orchestrate workflow
â”‚  â”œâ”€ Airflow / Dagster
â”‚  â”œâ”€ Dependencies
â”‚  â”œâ”€ Scheduling
â”‚  â””â”€ Error handling
â”‚
â”œâ”€ Monitor & maintain
â”‚  â”œâ”€ Data quality checks
â”‚  â”œâ”€ Pipeline health
â”‚  â”œâ”€ Performance metrics
â”‚  â””â”€ Alerting
â”‚
â””â”€ Scale & optimize
   â”œâ”€ Partitioning
   â”œâ”€ Parallel processing
   â”œâ”€ Caching
   â””â”€ Cost optimization
""",

    "Data Structures & Algorithms": """
Data Structures & Algorithms
â”œâ”€ Understand the problem
â”‚  â”œâ”€ Input format
â”‚  â”œâ”€ Output format
â”‚  â”œâ”€ Constraints
â”‚  â””â”€ Edge cases
â”‚
â”œâ”€ Choose data structure
â”‚  â”œâ”€ Array/List (sequential)
â”‚  â”œâ”€ Hash Map/Set (fast lookup)
â”‚  â”œâ”€ Stack/Queue (LIFO/FIFO)
â”‚  â”œâ”€ Tree/Graph (hierarchical)
â”‚  â””â”€ Heap (priority)
â”‚
â”œâ”€ Design algorithm
â”‚  â”œâ”€ Brute force first
â”‚  â”œâ”€ Identify pattern
â”‚  â”‚  â”œâ”€ Two pointers
â”‚  â”‚  â”œâ”€ Sliding window
â”‚  â”‚  â”œâ”€ BFS/DFS
â”‚  â”‚  â”œâ”€ Dynamic programming
â”‚  â”‚  â””â”€ Divide & conquer
â”‚  â””â”€ Optimize
â”‚
â”œâ”€ Implement
â”‚  â”œâ”€ Write clean code
â”‚  â”œâ”€ Handle edge cases
â”‚  â””â”€ Test with examples
â”‚
â””â”€ Analyze complexity
   â”œâ”€ Time: O(?)
   â”œâ”€ Space: O(?)
   â””â”€ Can we do better?
""",

    "System Design": """
System Design
â”œâ”€ Requirements (5 min)
â”‚  â”œâ”€ Functional requirements
â”‚  â”‚  â”œâ”€ What features?
â”‚  â”‚  â””â”€ What queries?
â”‚  â”œâ”€ Non-functional requirements
â”‚  â”‚  â”œâ”€ Scale (QPS, data volume)
â”‚  â”‚  â”œâ”€ Latency (ms, sec, min)
â”‚  â”‚  â””â”€ Availability (99.9%?)
â”‚  â””â”€ Constraints
â”‚
â”œâ”€ High-level design (10 min)
â”‚  â”œâ”€ Draw architecture
â”‚  â”œâ”€ Data flow
â”‚  â”œâ”€ Main components
â”‚  â””â”€ Technology choices
â”‚
â”œâ”€ Deep dive (20 min)
â”‚  â”œâ”€ Data ingestion
â”‚  â”‚  â”œâ”€ Batch or stream?
â”‚  â”‚  â”œâ”€ Message queue (Kafka)
â”‚  â”‚  â””â”€ Load balancer
â”‚  â”œâ”€ Data processing
â”‚  â”‚  â”œâ”€ Spark, Flink, Dataflow
â”‚  â”‚  â”œâ”€ Transformation logic
â”‚  â”‚  â””â”€ Aggregations
â”‚  â”œâ”€ Data storage
â”‚  â”‚  â”œâ”€ Data lake (S3, GCS)
â”‚  â”‚  â”œâ”€ Data warehouse (Snowflake, BigQuery)
â”‚  â”‚  â”œâ”€ Database (PostgreSQL, Cassandra)
â”‚  â”‚  â””â”€ Cache (Redis)
â”‚  â””â”€ Data serving
â”‚     â”œâ”€ APIs
â”‚     â”œâ”€ Dashboards
â”‚     â””â”€ ML models
â”‚
â”œâ”€ Scalability
â”‚  â”œâ”€ Horizontal scaling
â”‚  â”œâ”€ Partitioning/Sharding
â”‚  â”œâ”€ Replication
â”‚  â””â”€ Auto-scaling
â”‚
â”œâ”€ Reliability
â”‚  â”œâ”€ Fault tolerance
â”‚  â”œâ”€ Retry mechanisms
â”‚  â”œâ”€ Dead letter queue
â”‚  â””â”€ Disaster recovery
â”‚
â””â”€ Trade-offs
   â”œâ”€ Consistency vs Availability
   â”œâ”€ Latency vs Throughput
   â”œâ”€ Cost vs Performance
   â””â”€ Batch vs Stream
""",

    "Data Modeling": """
Data Modeling
â”œâ”€ Understand use cases
â”‚  â”œâ”€ What queries?
â”‚  â”œâ”€ Read vs write heavy?
â”‚  â”œâ”€ OLTP or OLAP?
â”‚  â””â”€ Users & access patterns
â”‚
â”œâ”€ Identify entities
â”‚  â”œâ”€ Business objects
â”‚  â”œâ”€ Relationships
â”‚  â””â”€ Attributes
â”‚
â”œâ”€ Choose approach
â”‚  â”œâ”€ Normalized (3NF)
â”‚  â”‚  â”œâ”€ Pros: No redundancy
â”‚  â”‚  â””â”€ Cons: Complex joins
â”‚  â”œâ”€ Denormalized (Star/Snowflake)
â”‚  â”‚  â”œâ”€ Pros: Fast reads
â”‚  â”‚  â””â”€ Cons: Data redundancy
â”‚  â””â”€ Hybrid
â”‚
â”œâ”€ Design schema
â”‚  â”œâ”€ Fact tables (metrics)
â”‚  â”œâ”€ Dimension tables (attributes)
â”‚  â”œâ”€ Primary keys
â”‚  â”œâ”€ Foreign keys
â”‚  â””â”€ Indexes
â”‚
â””â”€ Optimize
   â”œâ”€ Partitioning
   â”œâ”€ Clustering keys
   â”œâ”€ Materialized views
   â””â”€ Compression
""",

    "SQL": """
SQL
â”œâ”€ Understand requirements
â”‚  â”œâ”€ What output?
â”‚  â”œâ”€ Which tables?
â”‚  â”œâ”€ Filters/conditions?
â”‚  â””â”€ Aggregations needed?
â”‚
â”œâ”€ Plan query structure
â”‚  â”œâ”€ Identify joins
â”‚  â”œâ”€ Determine filters
â”‚  â”œâ”€ Plan aggregations
â”‚  â””â”€ Consider window functions
â”‚
â”œâ”€ Write query (use CTEs)
â”‚  â”œâ”€ WITH clause for readability
â”‚  â”œâ”€ SELECT columns
â”‚  â”œâ”€ FROM & JOIN
â”‚  â”œâ”€ WHERE (filters)
â”‚  â”œâ”€ GROUP BY
â”‚  â”œâ”€ HAVING
â”‚  â””â”€ ORDER BY + LIMIT
â”‚
â”œâ”€ Optimize
â”‚  â”œâ”€ Push down filters
â”‚  â”œâ”€ Use indexes
â”‚  â”œâ”€ Avoid SELECT *
â”‚  â”œâ”€ Partition pruning
â”‚  â””â”€ Check EXPLAIN plan
â”‚
â””â”€ Handle edge cases
   â”œâ”€ NULLs (COALESCE, IS NULL)
   â”œâ”€ Duplicates (DISTINCT)
   â”œâ”€ Empty results
   â””â”€ Data type conversions
""",

    "Data Warehousing": """
Data Warehousing
â”œâ”€ Architecture choice
â”‚  â”œâ”€ Traditional DW (on-prem)
â”‚  â”œâ”€ Cloud DW (Snowflake, Redshift, BigQuery)
â”‚  â”œâ”€ Data Lake (S3, ADLS, GCS)
â”‚  â””â”€ Data Lakehouse (Delta Lake, Iceberg)
â”‚
â”œâ”€ Schema design
â”‚  â”œâ”€ Star schema (1 fact, N dims)
â”‚  â”œâ”€ Snowflake schema (normalized)
â”‚  â”œâ”€ Fact tables (metrics)
â”‚  â”œâ”€ Dimension tables (attributes)
â”‚  â””â”€ SCD (Type 1/2/3)
â”‚
â”œâ”€ ETL vs ELT
â”‚  â”œâ”€ ETL: Transform before load
â”‚  â”œâ”€ ELT: Load then transform
â”‚  â””â”€ Choose based on use case
â”‚
â”œâ”€ Optimization
â”‚  â”œâ”€ Partitioning (date, region)
â”‚  â”œâ”€ Clustering (filter columns)
â”‚  â”œâ”€ Materialized views
â”‚  â”œâ”€ Compression (Parquet, ORC)
â”‚  â””â”€ Result caching
â”‚
â””â”€ Cost management
   â”œâ”€ Storage costs
   â”œâ”€ Compute costs
   â”œâ”€ Query optimization
   â””â”€ Data lifecycle policies
""",

    "Behavioral": """
Behavioral (STAR Method)
â”œâ”€ Situation
â”‚  â”œâ”€ Context
â”‚  â”œâ”€ Challenge
â”‚  â””â”€ Stakeholders
â”‚
â”œâ”€ Task
â”‚  â”œâ”€ Your role
â”‚  â”œâ”€ Goal
â”‚  â””â”€ Constraints
â”‚
â”œâ”€ Action
â”‚  â”œâ”€ What YOU did
â”‚  â”œâ”€ Technical decisions
â”‚  â”œâ”€ Trade-offs considered
â”‚  â””â”€ Collaboration
â”‚
â””â”€ Result
   â”œâ”€ Quantifiable impact
   â”œâ”€ What you learned
   â””â”€ What you'd do differently
""",

    "ETL/ELT": """
ETL/ELT
â”œâ”€ Extract
â”‚  â”œâ”€ Source systems (APIs, DBs, files)
â”‚  â”œâ”€ Full vs incremental
â”‚  â”œâ”€ Change Data Capture (CDC)
â”‚  â””â”€ Error handling
â”‚
â”œâ”€ Transform
â”‚  â”œâ”€ Data cleaning
â”‚  â”œâ”€ Data validation
â”‚  â”œâ”€ Business logic
â”‚  â”œâ”€ Aggregations
â”‚  â””â”€ Tools: Spark, dbt, Dataflow
â”‚
â”œâ”€ Load
â”‚  â”œâ”€ Append vs upsert
â”‚  â”œâ”€ Batch vs streaming
â”‚  â”œâ”€ Idempotency
â”‚  â””â”€ Error recovery
â”‚
â””â”€ Orchestration
   â”œâ”€ Airflow, Prefect, Dagster
   â”œâ”€ DAG design
   â”œâ”€ Scheduling
   â”œâ”€ Monitoring
   â””â”€ Alerting
""",

    "Data Quality": """
Data Quality
â”œâ”€ Define quality dimensions
â”‚  â”œâ”€ Accuracy
â”‚  â”œâ”€ Completeness
â”‚  â”œâ”€ Consistency
â”‚  â”œâ”€ Timeliness
â”‚  â”œâ”€ Validity
â”‚  â””â”€ Uniqueness
â”‚
â”œâ”€ Validation checks
â”‚  â”œâ”€ Schema validation
â”‚  â”œâ”€ Range checks
â”‚  â”œâ”€ Null checks
â”‚  â”œâ”€ Uniqueness checks
â”‚  â””â”€ Referential integrity
â”‚
â”œâ”€ Data profiling
â”‚  â”œâ”€ Statistical analysis
â”‚  â”œâ”€ Distribution analysis
â”‚  â”œâ”€ Outlier detection
â”‚  â””â”€ Pattern recognition
â”‚
â”œâ”€ Tools
â”‚  â”œâ”€ Great Expectations
â”‚  â”œâ”€ deequ (AWS)
â”‚  â”œâ”€ dbt tests
â”‚  â””â”€ Custom scripts
â”‚
â””â”€ Monitoring
   â”œâ”€ Data freshness SLAs
   â”œâ”€ Row count anomalies
   â”œâ”€ Schema drift
   â””â”€ Data lineage
""",

    "Spark/Big Data": """
Spark & Big Data
â”œâ”€ Understand Spark
â”‚  â”œâ”€ RDDs (low-level)
â”‚  â”œâ”€ DataFrames (structured)
â”‚  â”œâ”€ Datasets (typed)
â”‚  â””â”€ Lazy evaluation
â”‚
â”œâ”€ Transformations
â”‚  â”œâ”€ Narrow (map, filter)
â”‚  â”œâ”€ Wide (groupBy, join)
â”‚  â”œâ”€ Actions (collect, count, save)
â”‚  â””â”€ Catalyst optimizer
â”‚
â”œâ”€ Performance
â”‚  â”œâ”€ Partitioning
â”‚  â”œâ”€ Caching (persist, cache)
â”‚  â”œâ”€ Broadcast joins
â”‚  â”œâ”€ Avoid shuffles
â”‚  â””â”€ Tune executor memory/cores
â”‚
â”œâ”€ Data formats
â”‚  â”œâ”€ Parquet (columnar)
â”‚  â”œâ”€ Avro (row-based)
â”‚  â”œâ”€ ORC (optimized)
â”‚  â””â”€ JSON/CSV
â”‚
â””â”€ Cluster management
   â”œâ”€ YARN, Mesos, K8s
   â”œâ”€ Driver vs Executor
   â”œâ”€ Dynamic allocation
   â””â”€ Resource tuning
""",

    "Cloud Platforms": """
Cloud Platforms
â”œâ”€ AWS
â”‚  â”œâ”€ S3 (storage)
â”‚  â”œâ”€ Redshift (DW)
â”‚  â”œâ”€ Glue (ETL)
â”‚  â”œâ”€ Athena (SQL)
â”‚  â”œâ”€ EMR (Spark)
â”‚  â”œâ”€ Kinesis (streaming)
â”‚  â””â”€ Lambda (serverless)
â”‚
â”œâ”€ GCP
â”‚  â”œâ”€ GCS (storage)
â”‚  â”œâ”€ BigQuery (DW)
â”‚  â”œâ”€ Dataflow (ETL)
â”‚  â”œâ”€ Dataproc (Spark)
â”‚  â”œâ”€ Pub/Sub (messaging)
â”‚  â””â”€ Cloud Functions
â”‚
â””â”€ Azure
   â”œâ”€ ADLS (storage)
   â”œâ”€ Synapse (DW)
   â”œâ”€ Data Factory (ETL)
   â”œâ”€ Databricks (Spark)
   â”œâ”€ Event Hubs (streaming)
   â””â”€ Functions
""",

    "Coding": """
Coding
â”œâ”€ Understand problem
â”‚  â”œâ”€ Input format
â”‚  â”œâ”€ Output format
â”‚  â”œâ”€ Constraints
â”‚  â””â”€ Edge cases
â”‚
â”œâ”€ Design approach
â”‚  â”œâ”€ Brute force first
â”‚  â”œâ”€ Identify pattern
â”‚  â”œâ”€ Optimize
â”‚  â””â”€ Discuss trade-offs
â”‚
â”œâ”€ Implement
â”‚  â”œâ”€ Clean code
â”‚  â”œâ”€ Meaningful names
â”‚  â”œâ”€ Comments for complex logic
â”‚  â”œâ”€ Error handling
â”‚  â””â”€ Test as you go
â”‚
â””â”€ Analyze
   â”œâ”€ Time complexity
   â”œâ”€ Space complexity
   â””â”€ Can we optimize?
""",

    "Database Design": """
Database Design
â”œâ”€ Requirements
â”‚  â”œâ”€ OLTP or OLAP?
â”‚  â”œâ”€ Read vs write heavy?
â”‚  â”œâ”€ Query patterns
â”‚  â””â”€ Scale expectations
â”‚
â”œâ”€ Schema design
â”‚  â”œâ”€ Tables & columns
â”‚  â”œâ”€ Primary keys
â”‚  â”œâ”€ Foreign keys
â”‚  â”œâ”€ Indexes
â”‚  â””â”€ Constraints
â”‚
â”œâ”€ Normalization
â”‚  â”œâ”€ 1NF (atomic values)
â”‚  â”œâ”€ 2NF (no partial dependencies)
â”‚  â”œâ”€ 3NF (no transitive dependencies)
â”‚  â””â”€ When to denormalize
â”‚
â”œâ”€ Database type
â”‚  â”œâ”€ Relational (PostgreSQL, MySQL)
â”‚  â”œâ”€ NoSQL (MongoDB, Cassandra)
â”‚  â”œâ”€ Key-value (Redis, DynamoDB)
â”‚  â””â”€ Graph (Neo4j)
â”‚
â””â”€ Performance
   â”œâ”€ Indexing strategy
   â”œâ”€ Partitioning
   â”œâ”€ Replication
   â””â”€ Caching
""",

    "Monitoring & Observability": """
Monitoring & Observability
â”œâ”€ Metrics
â”‚  â”œâ”€ Pipeline success/failure
â”‚  â”œâ”€ Data volume processed
â”‚  â”œâ”€ Processing latency
â”‚  â”œâ”€ Resource utilization
â”‚  â””â”€ Cost metrics
â”‚
â”œâ”€ Logging
â”‚  â”œâ”€ Structured logs (JSON)
â”‚  â”œâ”€ Log levels
â”‚  â”œâ”€ Centralized logging
â”‚  â””â”€ Retention policies
â”‚
â”œâ”€ Alerting
â”‚  â”œâ”€ Data freshness SLAs
â”‚  â”œâ”€ Pipeline failures
â”‚  â”œâ”€ Data quality issues
â”‚  â””â”€ Anomaly detection
â”‚
â”œâ”€ Tracing
â”‚  â”œâ”€ Data lineage
â”‚  â”œâ”€ Distributed tracing
â”‚  â””â”€ Bottleneck identification
â”‚
â””â”€ Dashboards
   â”œâ”€ Real-time health
   â”œâ”€ Historical trends
   â””â”€ SLA compliance
""",

    "Distributed Systems": """
Distributed Systems
â”œâ”€ CAP Theorem
â”‚  â”œâ”€ Consistency
â”‚  â”œâ”€ Availability
â”‚  â””â”€ Partition Tolerance
â”‚
â”œâ”€ Consistency models
â”‚  â”œâ”€ Strong consistency
â”‚  â”œâ”€ Eventual consistency
â”‚  â””â”€ Causal consistency
â”‚
â”œâ”€ Replication
â”‚  â”œâ”€ Master-slave
â”‚  â”œâ”€ Multi-master
â”‚  â””â”€ Quorum-based
â”‚
â”œâ”€ Partitioning
â”‚  â”œâ”€ Hash-based
â”‚  â”œâ”€ Range-based
â”‚  â””â”€ Consistent hashing
â”‚
â””â”€ Fault tolerance
   â”œâ”€ Replication
   â”œâ”€ Checkpointing
   â””â”€ Retry mechanisms
""",

    "Streaming Data": """
Streaming Data
â”œâ”€ Streaming platforms
â”‚  â”œâ”€ Kafka
â”‚  â”œâ”€ Kinesis
â”‚  â”œâ”€ Pub/Sub
â”‚  â””â”€ Event Hubs
â”‚
â”œâ”€ Processing frameworks
â”‚  â”œâ”€ Spark Streaming
â”‚  â”œâ”€ Flink
â”‚  â”œâ”€ Storm
â”‚  â””â”€ Kafka Streams
â”‚
â”œâ”€ Windowing
â”‚  â”œâ”€ Tumbling (fixed)
â”‚  â”œâ”€ Sliding (overlapping)
â”‚  â”œâ”€ Session (gap-based)
â”‚  â””â”€ Watermarks
â”‚
â”œâ”€ State management
â”‚  â”œâ”€ Stateless operations
â”‚  â”œâ”€ Stateful operations
â”‚  â””â”€ Checkpointing
â”‚
â””â”€ Challenges
   â”œâ”€ Out-of-order events
   â”œâ”€ Late arrivals
   â”œâ”€ Exactly-once semantics
   â””â”€ Backpressure
""",

    "Performance Optimization": """
Performance Optimization
â”œâ”€ Identify bottlenecks
â”‚  â”œâ”€ Profiling tools
â”‚  â”œâ”€ Query execution plans
â”‚  â”œâ”€ Resource monitoring
â”‚  â””â”€ Slow query logs
â”‚
â”œâ”€ Query optimization
â”‚  â”œâ”€ Use indexes
â”‚  â”œâ”€ Push down filters
â”‚  â”œâ”€ Avoid SELECT *
â”‚  â”œâ”€ Partition pruning
â”‚  â””â”€ Materialized views
â”‚
â”œâ”€ Data optimization
â”‚  â”œâ”€ Compression
â”‚  â”œâ”€ Columnar storage
â”‚  â”œâ”€ Partitioning
â”‚  â””â”€ Clustering
â”‚
â”œâ”€ Compute optimization
â”‚  â”œâ”€ Increase parallelism
â”‚  â”œâ”€ Tune memory/CPU
â”‚  â”œâ”€ Caching
â”‚  â””â”€ Broadcast joins
â”‚
â””â”€ Architecture optimization
   â”œâ”€ Denormalize for reads
   â”œâ”€ Pre-aggregate data
   â”œâ”€ Use CDNs
   â””â”€ Horizontal scaling
""",

    "Schema Design": """
Schema Design
â”œâ”€ Schema evolution
â”‚  â”œâ”€ Add columns (backward compatible)
â”‚  â”œâ”€ Remove columns (forward compatible)
â”‚  â”œâ”€ Change types (breaking)
â”‚  â””â”€ Schema registry
â”‚
â”œâ”€ Data types
â”‚  â”œâ”€ Numeric (INT, BIGINT, DECIMAL)
â”‚  â”œâ”€ String (VARCHAR, TEXT)
â”‚  â”œâ”€ Date/Time (DATE, TIMESTAMP)
â”‚  â”œâ”€ Boolean
â”‚  â””â”€ JSON/JSONB
â”‚
â”œâ”€ Constraints
â”‚  â”œâ”€ NOT NULL
â”‚  â”œâ”€ UNIQUE
â”‚  â”œâ”€ PRIMARY KEY
â”‚  â”œâ”€ FOREIGN KEY
â”‚  â””â”€ CHECK
â”‚
â””â”€ Best practices
   â”œâ”€ Appropriate types
   â”œâ”€ Avoid nullable keys
   â”œâ”€ Document schema
   â””â”€ Version schemas
""",

    "Data Governance": """
Data Governance
â”œâ”€ Data privacy
â”‚  â”œâ”€ PII handling
â”‚  â”œâ”€ GDPR compliance
â”‚  â”œâ”€ CCPA compliance
â”‚  â”œâ”€ Anonymization
â”‚  â””â”€ Data masking
â”‚
â”œâ”€ Data security
â”‚  â”œâ”€ Encryption at rest
â”‚  â”œâ”€ Encryption in transit
â”‚  â”œâ”€ Access control (IAM, RBAC)
â”‚  â”œâ”€ Audit logs
â”‚  â””â”€ Retention policies
â”‚
â”œâ”€ Data quality
â”‚  â”œâ”€ Validation
â”‚  â”œâ”€ Profiling
â”‚  â””â”€ Lineage
â”‚
â”œâ”€ Metadata management
â”‚  â”œâ”€ Data catalog
â”‚  â”œâ”€ Schema registry
â”‚  â”œâ”€ Data dictionary
â”‚  â””â”€ Tags & classifications
â”‚
â””â”€ Compliance
   â”œâ”€ Regulatory requirements
   â”œâ”€ Data residency
   â”œâ”€ Right to be forgotten
   â””â”€ Audit trails
"""
}

TESTING_EXPLANATIONS = {
    "Data Pipeline Design": "Can you design robust, scalable data pipelines from source to destination?",
    "Data Structures & Algorithms": "Can you write efficient code to process and manipulate data at scale?",
    "System Design": "Can you architect data systems that handle TB/PB scale with high availability?",
    "Data Modeling": "Can you design schemas that are efficient, maintainable, and support business queries?",
    "SQL": "Can you write complex, optimized SQL queries for data transformation and analysis?",
    "Data Warehousing": "Do you understand modern data warehouse architectures and best practices?",
    "Behavioral": "Can you work effectively in a team and handle challenges?",
    "ETL/ELT": "Do you understand data transformation workflows and orchestration?",
    "Data Quality": "Can you ensure data reliability and trustworthiness?",
    "Spark/Big Data": "Can you process large-scale data efficiently using distributed computing?",
    "Cloud Platforms": "Are you familiar with cloud-native data services?",
    "Coding": "Can you write clean, efficient Python/Scala code for data processing?",
    "Database Design": "Can you design database schemas for different use cases?",
    "Monitoring & Observability": "Can you ensure pipeline reliability and quickly debug issues?",
    "Distributed Systems": "Do you understand distributed computing principles?",
    "Streaming Data": "Can you process real-time data streams?",
    "Performance Optimization": "Can you identify and fix performance bottlenecks?",
    "Schema Design": "Can you design flexible, maintainable schemas?",
    "Data Governance": "Do you understand data privacy, security, and compliance?"
}

# Generate Question Bank
output = []
output.append("")
output.append("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
output.append("â•‘                                                                                â•‘")
output.append("â•‘          DATA ENGINEER INTERVIEW PREPARATION FRAMEWORK                         â•‘")
output.append("â•‘          Mental Models & Complete Question Bank (#1 GOAL ROLE)                 â•‘")
output.append("â•‘                                                                                â•‘")
output.append("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
output.append("")
output.append("This framework provides comprehensive mental models for approaching each type of data")
output.append("engineer interview question. This is your #1 goal role - master these patterns!")
output.append("")
output.append("Focus on understanding the PATTERN and FRAMEWORK, not memorizing answers.")
output.append("")
output.append(f"Total Questions: {sum(len(qs) for qs in questions_by_category.values())} across {len(questions_by_category)} categories")
output.append("")
output.append("")

# Sort categories by question count (descending)
sorted_categories = sorted(questions_by_category.items(), key=lambda x: len(x[1]), reverse=True)

for category, questions in sorted_categories:
    output.append("=" * 80)
    output.append(category.upper())
    output.append("=" * 80)
    output.append("")
    output.append(f"ğŸ“Š Total Questions: {len(questions)}")
    output.append("")
    output.append("ğŸ¯ What they're really testing:")
    output.append(TESTING_EXPLANATIONS.get(category, "Can you handle this type of question effectively?"))
    output.append("")
    output.append("ğŸ—ºï¸  Mental Model Framework:")
    output.append("```")
    output.append(FRAMEWORKS.get(category, "Framework coming soon..."))
    output.append("```")
    output.append("")
    output.append(f"ğŸ“ All {len(questions)} Questions:")
    output.append("")
    
    for i, q in enumerate(questions, 1):
        output.append(f"{i}. {q['question']}")
    
    output.append("")
    output.append("")

# Write to file
output_file = 'Data_Engineer_Question_Bank.md'
with open(output_file, 'w') as f:
    f.write('\n'.join(output))

print(f"âœ… Generated {output_file}")
print(f"ğŸ“Š Total: {sum(len(qs) for qs in questions_by_category.values())} questions across {len(questions_by_category)} categories")
