"""
Generate comprehensive Data Scientist frameworks matching Data Analyst quality
"""
import json
import os

def get_framework_for_category(category):
    """Return comprehensive ASCII framework for each DS category"""
    
    frameworks = {
        "Behavioral": """
Behavioral (STAR Method)
‚îú‚îÄ Situation
‚îÇ  ‚îú‚îÄ Context and background
‚îÇ  ‚îú‚îÄ Business metrics at the time
‚îÇ  ‚îú‚îÄ Team and stakeholders
‚îÇ  ‚îî‚îÄ Why this was important
‚îÇ
‚îú‚îÄ Task
‚îÇ  ‚îú‚îÄ Your specific responsibility
‚îÇ  ‚îú‚îÄ Goals and objectives
‚îÇ  ‚îú‚îÄ Constraints (time, resources, data)
‚îÇ  ‚îî‚îÄ Success criteria
‚îÇ
‚îú‚îÄ Action
‚îÇ  ‚îú‚îÄ Approach and methodology
‚îÇ  ‚îú‚îÄ Models/techniques used
‚îÇ  ‚îú‚îÄ How you collaborated
‚îÇ  ‚îú‚îÄ Challenges you overcame
‚îÇ  ‚îî‚îÄ Iterations and improvements
‚îÇ
‚îî‚îÄ Result
   ‚îú‚îÄ Quantifiable outcomes
   ‚îú‚îÄ Business impact (revenue, efficiency, etc.)
   ‚îú‚îÄ Model performance metrics
   ‚îú‚îÄ What you learned
   ‚îî‚îÄ How you'd apply it again
""",
        "Machine Learning - Supervised": """
Machine Learning - Supervised
‚îú‚îÄ Understand the problem
‚îÇ  ‚îú‚îÄ Business objective
‚îÇ  ‚îú‚îÄ Prediction task (regression/classification)
‚îÇ  ‚îú‚îÄ Success metrics
‚îÇ  ‚îî‚îÄ Constraints
‚îÇ
‚îú‚îÄ Data preparation
‚îÇ  ‚îú‚îÄ Feature engineering
‚îÇ  ‚îú‚îÄ Handle missing data
‚îÇ  ‚îú‚îÄ Encode categorical variables
‚îÇ  ‚îî‚îÄ Train/validation/test split
‚îÇ
‚îú‚îÄ Model selection
‚îÇ  ‚îú‚îÄ Linear models (regression, logistic)
‚îÇ  ‚îú‚îÄ Tree-based (RF, XGBoost, etc.)
‚îÇ  ‚îú‚îÄ Neural networks
‚îÇ  ‚îî‚îÄ Consider complexity vs interpretability
‚îÇ
‚îú‚îÄ Training and tuning
‚îÇ  ‚îú‚îÄ Hyperparameter optimization
‚îÇ  ‚îú‚îÄ Cross-validation
‚îÇ  ‚îú‚îÄ Regularization (L1/L2)
‚îÇ  ‚îî‚îÄ Monitor for overfitting
‚îÇ
‚îî‚îÄ Evaluation
   ‚îú‚îÄ Appropriate metrics (accuracy, precision, recall, RMSE, etc.)
   ‚îú‚îÄ Confusion matrix / ROC curve
   ‚îú‚îÄ Feature importance
   ‚îî‚îÄ Business impact
""",
        "Machine Learning - Unsupervised": """
Machine Learning - Unsupervised
‚îú‚îÄ Define objective
‚îÇ  ‚îú‚îÄ What patterns to discover?
‚îÇ  ‚îú‚îÄ Clustering or dimensionality reduction?
‚îÇ  ‚îî‚îÄ How will results be used?
‚îÇ
‚îú‚îÄ Data preparation
‚îÇ  ‚îú‚îÄ Feature scaling/normalization
‚îÇ  ‚îú‚îÄ Handle missing values
‚îÇ  ‚îú‚îÄ Remove outliers (if appropriate)
‚îÇ  ‚îî‚îÄ Feature selection
‚îÇ
‚îú‚îÄ Algorithm selection
‚îÇ  ‚îú‚îÄ Clustering: K-means, hierarchical, DBSCAN
‚îÇ  ‚îú‚îÄ Dimensionality reduction: PCA, t-SNE, UMAP
‚îÇ  ‚îî‚îÄ Consider data characteristics
‚îÇ
‚îú‚îÄ Model training
‚îÇ  ‚îú‚îÄ Choose number of clusters/components
‚îÇ  ‚îú‚îÄ Initialize properly
‚îÇ  ‚îú‚îÄ Iterate and refine
‚îÇ  ‚îî‚îÄ Validate stability
‚îÇ
‚îî‚îÄ Evaluation and interpretation
   ‚îú‚îÄ Silhouette score, elbow method
   ‚îú‚îÄ Visualize results
   ‚îú‚îÄ Interpret clusters/components
   ‚îî‚îÄ Validate with domain knowledge
""",
        "Machine Learning - Model Evaluation": """
Machine Learning - Model Evaluation
‚îú‚îÄ Choose appropriate metrics
‚îÇ  ‚îú‚îÄ Classification: accuracy, precision, recall, F1, AUC-ROC
‚îÇ  ‚îú‚îÄ Regression: RMSE, MAE, R¬≤, MAPE
‚îÇ  ‚îú‚îÄ Consider business context
‚îÇ  ‚îî‚îÄ Class imbalance considerations
‚îÇ
‚îú‚îÄ Cross-validation
‚îÇ  ‚îú‚îÄ K-fold cross-validation
‚îÇ  ‚îú‚îÄ Stratified for imbalanced data
‚îÇ  ‚îú‚îÄ Time series: forward chaining
‚îÇ  ‚îî‚îÄ Report mean and std of metrics
‚îÇ
‚îú‚îÄ Bias-variance tradeoff
‚îÇ  ‚îú‚îÄ Underfitting (high bias)
‚îÇ  ‚îú‚îÄ Overfitting (high variance)
‚îÇ  ‚îú‚îÄ Learning curves
‚îÇ  ‚îî‚îÄ Regularization strategies
‚îÇ
‚îú‚îÄ Error analysis
‚îÇ  ‚îú‚îÄ Confusion matrix
‚îÇ  ‚îú‚îÄ Analyze misclassifications
‚îÇ  ‚îú‚îÄ Feature importance
‚îÇ  ‚îî‚îÄ Identify systematic errors
‚îÇ
‚îî‚îÄ Production considerations
   ‚îú‚îÄ Model robustness
   ‚îú‚îÄ Inference latency
   ‚îú‚îÄ Model size
   ‚îî‚îÄ Monitoring and retraining
""",
        "Machine Learning - Feature Engineering": """
Machine Learning - Feature Engineering
‚îú‚îÄ Understand the data
‚îÇ  ‚îú‚îÄ Domain knowledge
‚îÇ  ‚îú‚îÄ Data types and distributions
‚îÇ  ‚îú‚îÄ Missing values and outliers
‚îÇ  ‚îî‚îÄ Correlations
‚îÇ
‚îú‚îÄ Create new features
‚îÇ  ‚îú‚îÄ Interactions (A * B, A / B)
‚îÇ  ‚îú‚îÄ Aggregations (sum, mean, count)
‚îÇ  ‚îú‚îÄ Time-based (day of week, hour, lag)
‚îÇ  ‚îú‚îÄ Text features (TF-IDF, embeddings)
‚îÇ  ‚îî‚îÄ Domain-specific transformations
‚îÇ
‚îú‚îÄ Transform existing features
‚îÇ  ‚îú‚îÄ Scaling (standardization, normalization)
‚îÇ  ‚îú‚îÄ Encoding (one-hot, label, target)
‚îÇ  ‚îú‚îÄ Binning/discretization
‚îÇ  ‚îî‚îÄ Log/power transformations
‚îÇ
‚îú‚îÄ Feature selection
‚îÇ  ‚îú‚îÄ Remove low variance features
‚îÇ  ‚îú‚îÄ Remove highly correlated features
‚îÇ  ‚îú‚îÄ Feature importance from models
‚îÇ  ‚îú‚îÄ Recursive feature elimination
‚îÇ  ‚îî‚îÄ L1 regularization
‚îÇ
‚îî‚îÄ Validate
   ‚îú‚îÄ Check for data leakage
   ‚îú‚îÄ Validate on holdout set
   ‚îú‚îÄ Monitor feature drift
   ‚îî‚îÄ Document feature definitions
""",
        "Statistics & Experimentation - A/B Testing": """
Statistics & Experimentation - A/B Testing
‚îú‚îÄ Design the experiment
‚îÇ  ‚îú‚îÄ Define hypothesis (null and alternative)
‚îÇ  ‚îú‚îÄ Choose primary metric
‚îÇ  ‚îú‚îÄ Secondary and guardrail metrics
‚îÇ  ‚îú‚îÄ Determine sample size (power analysis)
‚îÇ  ‚îî‚îÄ Randomization strategy
‚îÇ
‚îú‚îÄ Run the test
‚îÇ  ‚îú‚îÄ Ensure proper randomization
‚îÇ  ‚îú‚îÄ Monitor for issues (SRM, bugs)
‚îÇ  ‚îú‚îÄ Avoid peeking
‚îÇ  ‚îî‚îÄ Collect sufficient data
‚îÇ
‚îú‚îÄ Analyze results
‚îÇ  ‚îú‚îÄ Calculate statistical significance (p-value)
‚îÇ  ‚îú‚îÄ Calculate confidence intervals
‚îÇ  ‚îú‚îÄ Check for practical significance
‚îÇ  ‚îú‚îÄ Segment analysis
‚îÇ  ‚îî‚îÄ Check guardrail metrics
‚îÇ
‚îú‚îÄ Interpret findings
‚îÇ  ‚îú‚îÄ Can we reject null hypothesis?
‚îÇ  ‚îú‚îÄ Effect size and business impact
‚îÇ  ‚îú‚îÄ Consider external factors
‚îÇ  ‚îî‚îÄ Long-term vs short-term effects
‚îÇ
‚îî‚îÄ Make decision
   ‚îú‚îÄ Ship, iterate, or kill
   ‚îú‚îÄ Document learnings
   ‚îî‚îÄ Plan next experiments
""",
        "Statistics & Experimentation - Hypothesis Testing": """
Statistics & Experimentation - Hypothesis Testing
‚îú‚îÄ Formulate hypotheses
‚îÇ  ‚îú‚îÄ Null hypothesis (H0)
‚îÇ  ‚îú‚îÄ Alternative hypothesis (H1)
‚îÇ  ‚îî‚îÄ One-tailed vs two-tailed
‚îÇ
‚îú‚îÄ Choose test
‚îÇ  ‚îú‚îÄ t-test (means, small samples)
‚îÇ  ‚îú‚îÄ z-test (means, large samples)
‚îÇ  ‚îú‚îÄ Chi-square (categorical)
‚îÇ  ‚îú‚îÄ ANOVA (multiple groups)
‚îÇ  ‚îî‚îÄ Non-parametric alternatives
‚îÇ
‚îú‚îÄ Set significance level
‚îÇ  ‚îú‚îÄ Alpha (typically 0.05)
‚îÇ  ‚îú‚îÄ Type I error (false positive)
‚îÇ  ‚îú‚îÄ Type II error (false negative)
‚îÇ  ‚îî‚îÄ Power (1 - Type II error)
‚îÇ
‚îú‚îÄ Calculate test statistic
‚îÇ  ‚îú‚îÄ Compute from sample data
‚îÇ  ‚îú‚îÄ Compare to null distribution
‚îÇ  ‚îî‚îÄ Calculate p-value
‚îÇ
‚îî‚îÄ Make decision
   ‚îú‚îÄ Reject or fail to reject H0
   ‚îú‚îÄ Confidence interval
   ‚îú‚îÄ Practical significance
   ‚îî‚îÄ Communicate findings
""",
        "Statistics & Experimentation - Probability": """
Statistics & Experimentation - Probability
‚îú‚îÄ Understand the problem
‚îÇ  ‚îú‚îÄ What are we trying to find?
‚îÇ  ‚îú‚îÄ What information is given?
‚îÇ  ‚îî‚îÄ What assumptions can we make?
‚îÇ
‚îú‚îÄ Identify distribution
‚îÇ  ‚îú‚îÄ Discrete: binomial, Poisson, geometric
‚îÇ  ‚îú‚îÄ Continuous: normal, exponential, uniform
‚îÇ  ‚îú‚îÄ Parameters (mean, variance, etc.)
‚îÇ  ‚îî‚îÄ Assumptions and conditions
‚îÇ
‚îú‚îÄ Apply probability rules
‚îÇ  ‚îú‚îÄ Addition rule (OR)
‚îÇ  ‚îú‚îÄ Multiplication rule (AND)
‚îÇ  ‚îú‚îÄ Conditional probability
‚îÇ  ‚îú‚îÄ Bayes' theorem
‚îÇ  ‚îî‚îÄ Independence
‚îÇ
‚îú‚îÄ Calculate
‚îÇ  ‚îú‚îÄ Expected value
‚îÇ  ‚îú‚îÄ Variance and standard deviation
‚îÇ  ‚îú‚îÄ Confidence intervals
‚îÇ  ‚îî‚îÄ Percentiles
‚îÇ
‚îî‚îÄ Interpret
   ‚îú‚îÄ What does the result mean?
   ‚îú‚îÄ Practical implications
   ‚îî‚îÄ Uncertainty and assumptions
""",
        "Data Analysis - Root Cause": """
Data Analysis - Root Cause
‚îú‚îÄ Define the problem
‚îÇ  ‚îú‚îÄ What metric changed?
‚îÇ  ‚îú‚îÄ When did it change?
‚îÇ  ‚îú‚îÄ How much did it change?
‚îÇ  ‚îî‚îÄ Why does it matter?
‚îÇ
‚îú‚îÄ Form hypotheses
‚îÇ  ‚îú‚îÄ Internal factors (product changes, bugs)
‚îÇ  ‚îú‚îÄ External factors (seasonality, competition)
‚îÇ  ‚îú‚îÄ User behavior changes
‚îÇ  ‚îî‚îÄ Data quality issues
‚îÇ
‚îú‚îÄ Segment and drill down
‚îÇ  ‚îú‚îÄ By time (hourly, daily, weekly)
‚îÇ  ‚îú‚îÄ By user cohort (new vs returning)
‚îÇ  ‚îú‚îÄ By platform (iOS, Android, web)
‚îÇ  ‚îú‚îÄ By geography
‚îÇ  ‚îî‚îÄ By feature usage
‚îÇ
‚îú‚îÄ Test hypotheses
‚îÇ  ‚îú‚îÄ Gather supporting data
‚îÇ  ‚îú‚îÄ Rule out alternatives
‚îÇ  ‚îú‚îÄ Look for correlations
‚îÇ  ‚îî‚îÄ Identify root cause
‚îÇ
‚îî‚îÄ Recommend action
   ‚îú‚îÄ Fix the issue
   ‚îú‚îÄ Prevent recurrence
   ‚îú‚îÄ Monitor going forward
   ‚îî‚îÄ Expected impact
""",
        "Data Analysis - Business Metrics": """
Data Analysis - Business Metrics
‚îú‚îÄ Understand the business
‚îÇ  ‚îú‚îÄ Business model
‚îÇ  ‚îú‚îÄ Key value drivers
‚îÇ  ‚îú‚îÄ User journey
‚îÇ  ‚îî‚îÄ Competitive landscape
‚îÇ
‚îú‚îÄ Define metrics
‚îÇ  ‚îú‚îÄ North Star Metric
‚îÇ  ‚îú‚îÄ Leading indicators
‚îÇ  ‚îú‚îÄ Lagging indicators
‚îÇ  ‚îú‚îÄ Input vs output metrics
‚îÇ  ‚îî‚îÄ Guardrail metrics
‚îÇ
‚îú‚îÄ Measure and track
‚îÇ  ‚îú‚îÄ Data sources
‚îÇ  ‚îú‚îÄ Calculation methodology
‚îÇ  ‚îú‚îÄ Frequency of measurement
‚îÇ  ‚îú‚îÄ Dashboards and reports
‚îÇ  ‚îî‚îÄ Alerts and thresholds
‚îÇ
‚îú‚îÄ Analyze trends
‚îÇ  ‚îú‚îÄ Historical patterns
‚îÇ  ‚îú‚îÄ Seasonality
‚îÇ  ‚îú‚îÄ Growth rates
‚îÇ  ‚îú‚îÄ Cohort analysis
‚îÇ  ‚îî‚îÄ Segment comparisons
‚îÇ
‚îî‚îÄ Drive action
   ‚îú‚îÄ Insights and recommendations
   ‚îú‚îÄ Prioritize initiatives
   ‚îú‚îÄ Set targets and goals
   ‚îî‚îÄ Measure impact
""",
        "SQL": """
SQL
‚îú‚îÄ Understand requirements
‚îÇ  ‚îú‚îÄ What question are we answering?
‚îÇ  ‚îú‚îÄ What tables are involved?
‚îÇ  ‚îú‚îÄ What's the grain of the output?
‚îÇ  ‚îî‚îÄ Performance considerations
‚îÇ
‚îú‚îÄ Write the query
‚îÇ  ‚îú‚îÄ SELECT appropriate columns
‚îÇ  ‚îú‚îÄ FROM and JOIN tables
‚îÇ  ‚îú‚îÄ WHERE to filter rows
‚îÇ  ‚îú‚îÄ GROUP BY for aggregations
‚îÇ  ‚îú‚îÄ HAVING to filter groups
‚îÇ  ‚îî‚îÄ ORDER BY and LIMIT
‚îÇ
‚îú‚îÄ Use advanced features
‚îÇ  ‚îú‚îÄ Window functions (ROW_NUMBER, RANK, LAG, LEAD)
‚îÇ  ‚îú‚îÄ CTEs (WITH clause) for readability
‚îÇ  ‚îú‚îÄ Subqueries
‚îÇ  ‚îú‚îÄ CASE statements
‚îÇ  ‚îî‚îÄ Date functions
‚îÇ
‚îú‚îÄ Optimize
‚îÇ  ‚îú‚îÄ Use indexes effectively
‚îÇ  ‚îú‚îÄ Avoid SELECT *
‚îÇ  ‚îú‚îÄ Filter early (WHERE before JOIN)
‚îÇ  ‚îú‚îÄ Limit result set
‚îÇ  ‚îî‚îÄ Explain plan
‚îÇ
‚îî‚îÄ Validate
   ‚îú‚îÄ Check for nulls
   ‚îú‚îÄ Verify row counts
   ‚îú‚îÄ Spot check results
   ‚îî‚îÄ Test edge cases
""",
        "Coding": """
Coding
‚îú‚îÄ Understand the problem
‚îÇ  ‚îú‚îÄ Read carefully
‚îÇ  ‚îú‚îÄ Ask clarifying questions
‚îÇ  ‚îú‚îÄ Identify inputs and outputs
‚îÇ  ‚îú‚îÄ Constraints and edge cases
‚îÇ  ‚îî‚îÄ Examples
‚îÇ
‚îú‚îÄ Plan the approach
‚îÇ  ‚îú‚îÄ Brute force first
‚îÇ  ‚îú‚îÄ Identify patterns
‚îÇ  ‚îú‚îÄ Choose data structures
‚îÇ  ‚îú‚îÄ Consider time/space complexity
‚îÇ  ‚îî‚îÄ Outline algorithm
‚îÇ
‚îú‚îÄ Implement
‚îÇ  ‚îú‚îÄ Write clean, readable code
‚îÇ  ‚îú‚îÄ Use meaningful variable names
‚îÇ  ‚îú‚îÄ Handle edge cases
‚îÇ  ‚îú‚îÄ Add comments for clarity
‚îÇ  ‚îî‚îÄ Test as you go
‚îÇ
‚îú‚îÄ Test
‚îÇ  ‚îú‚îÄ Normal cases
‚îÇ  ‚îú‚îÄ Edge cases (empty, single element)
‚îÇ  ‚îú‚îÄ Large inputs
‚îÇ  ‚îî‚îÄ Invalid inputs
‚îÇ
‚îî‚îÄ Optimize
   ‚îú‚îÄ Time complexity
   ‚îú‚îÄ Space complexity
   ‚îú‚îÄ Code readability
   ‚îî‚îÄ Discuss tradeoffs
""",
        "Model Deployment & Production": """
Model Deployment & Production
‚îú‚îÄ Prepare for deployment
‚îÇ  ‚îú‚îÄ Model serialization (pickle, joblib, ONNX)
‚îÇ  ‚îú‚îÄ Dependency management
‚îÇ  ‚îú‚îÄ Version control
‚îÇ  ‚îî‚îÄ Documentation
‚îÇ
‚îú‚îÄ Deploy
‚îÇ  ‚îú‚îÄ Batch vs real-time
‚îÇ  ‚îú‚îÄ API endpoint (REST, gRPC)
‚îÇ  ‚îú‚îÄ Containerization (Docker)
‚îÇ  ‚îú‚îÄ Orchestration (Kubernetes)
‚îÇ  ‚îî‚îÄ A/B testing framework
‚îÇ
‚îú‚îÄ Monitor
‚îÇ  ‚îú‚îÄ Model performance metrics
‚îÇ  ‚îú‚îÄ Prediction latency
‚îÇ  ‚îú‚îÄ Data drift
‚îÇ  ‚îú‚îÄ Concept drift
‚îÇ  ‚îú‚îÄ Error rates
‚îÇ  ‚îî‚îÄ Resource usage
‚îÇ
‚îú‚îÄ Maintain
‚îÇ  ‚îú‚îÄ Retrain schedule
‚îÇ  ‚îú‚îÄ Feature store
‚îÇ  ‚îú‚îÄ Model registry
‚îÇ  ‚îú‚îÄ Rollback strategy
‚îÇ  ‚îî‚îÄ Incident response
‚îÇ
‚îî‚îÄ Iterate
   ‚îú‚îÄ Collect feedback
   ‚îú‚îÄ Analyze failures
   ‚îú‚îÄ Improve features
   ‚îî‚îÄ Update model
""",
        "Deep Learning": """
Deep Learning
‚îú‚îÄ Problem formulation
‚îÇ  ‚îú‚îÄ Task type (classification, regression, generation)
‚îÇ  ‚îú‚îÄ Data availability
‚îÇ  ‚îú‚îÄ Computational resources
‚îÇ  ‚îî‚îÄ Interpretability needs
‚îÇ
‚îú‚îÄ Architecture design
‚îÇ  ‚îú‚îÄ Input layer (shape, preprocessing)
‚îÇ  ‚îú‚îÄ Hidden layers (CNN, RNN, Transformer)
‚îÇ  ‚îú‚îÄ Activation functions (ReLU, tanh, sigmoid)
‚îÇ  ‚îú‚îÄ Output layer (softmax, linear)
‚îÇ  ‚îî‚îÄ Number of parameters
‚îÇ
‚îú‚îÄ Training
‚îÇ  ‚îú‚îÄ Loss function
‚îÇ  ‚îú‚îÄ Optimizer (Adam, SGD)
‚îÇ  ‚îú‚îÄ Learning rate schedule
‚îÇ  ‚îú‚îÄ Batch size
‚îÇ  ‚îú‚îÄ Regularization (dropout, batch norm)
‚îÇ  ‚îî‚îÄ Early stopping
‚îÇ
‚îú‚îÄ Evaluation
‚îÇ  ‚îú‚îÄ Validation metrics
‚îÇ  ‚îú‚îÄ Learning curves
‚îÇ  ‚îú‚îÄ Overfitting/underfitting
‚îÇ  ‚îî‚îÄ Generalization
‚îÇ
‚îî‚îÄ Optimization
   ‚îú‚îÄ Hyperparameter tuning
   ‚îú‚îÄ Data augmentation
   ‚îú‚îÄ Transfer learning
   ‚îî‚îÄ Model compression
""",
        "Time Series": """
Time Series
‚îú‚îÄ Understand the data
‚îÇ  ‚îú‚îÄ Trend
‚îÇ  ‚îú‚îÄ Seasonality
‚îÇ  ‚îú‚îÄ Cyclical patterns
‚îÇ  ‚îú‚îÄ Irregularities/outliers
‚îÇ  ‚îî‚îÄ Stationarity
‚îÇ
‚îú‚îÄ Preprocessing
‚îÇ  ‚îú‚îÄ Handle missing values
‚îÇ  ‚îú‚îÄ Detrending
‚îÇ  ‚îú‚îÄ Deseasonalizing
‚îÇ  ‚îú‚îÄ Differencing
‚îÇ  ‚îî‚îÄ Scaling
‚îÇ
‚îú‚îÄ Model selection
‚îÇ  ‚îú‚îÄ Classical: ARIMA, SARIMA, Exponential Smoothing
‚îÇ  ‚îú‚îÄ ML: Random Forest, XGBoost with lag features
‚îÇ  ‚îú‚îÄ DL: LSTM, GRU, Transformer
‚îÇ  ‚îî‚îÄ Consider forecast horizon
‚îÇ
‚îú‚îÄ Feature engineering
‚îÇ  ‚îú‚îÄ Lag features
‚îÇ  ‚îú‚îÄ Rolling statistics
‚îÇ  ‚îú‚îÄ Time-based features (day of week, month)
‚îÇ  ‚îî‚îÄ External variables
‚îÇ
‚îî‚îÄ Evaluation
   ‚îú‚îÄ Train/validation/test split (time-based)
   ‚îú‚îÄ Metrics: MAE, RMSE, MAPE
   ‚îú‚îÄ Forecast vs actual plots
   ‚îî‚îÄ Residual analysis
""",
        "NLP": """
Natural Language Processing
‚îú‚îÄ Text preprocessing
‚îÇ  ‚îú‚îÄ Tokenization
‚îÇ  ‚îú‚îÄ Lowercasing
‚îÇ  ‚îú‚îÄ Remove stopwords
‚îÇ  ‚îú‚îÄ Stemming/lemmatization
‚îÇ  ‚îî‚îÄ Handle special characters
‚îÇ
‚îú‚îÄ Feature extraction
‚îÇ  ‚îú‚îÄ Bag of words
‚îÇ  ‚îú‚îÄ TF-IDF
‚îÇ  ‚îú‚îÄ Word embeddings (Word2Vec, GloVe)
‚îÇ  ‚îú‚îÄ Contextual embeddings (BERT, GPT)
‚îÇ  ‚îî‚îÄ Character-level features
‚îÇ
‚îú‚îÄ Model selection
‚îÇ  ‚îú‚îÄ Classical: Naive Bayes, SVM, Logistic Regression
‚îÇ  ‚îú‚îÄ Deep learning: RNN, LSTM, Transformer
‚îÇ  ‚îú‚îÄ Pre-trained models: BERT, RoBERTa, GPT
‚îÇ  ‚îî‚îÄ Task-specific architectures
‚îÇ
‚îú‚îÄ Training
‚îÇ  ‚îú‚îÄ Handle class imbalance
‚îÇ  ‚îú‚îÄ Sequence padding/truncation
‚îÇ  ‚îú‚îÄ Fine-tuning strategies
‚îÇ  ‚îî‚îÄ Regularization
‚îÇ
‚îî‚îÄ Evaluation
   ‚îú‚îÄ Accuracy, precision, recall, F1
   ‚îú‚îÄ Confusion matrix
   ‚îú‚îÄ Error analysis
   ‚îî‚îÄ Qualitative review
""",
        "Computer Vision": """
Computer Vision
‚îú‚îÄ Problem definition
‚îÇ  ‚îú‚îÄ Classification, detection, segmentation
‚îÇ  ‚îú‚îÄ Data availability
‚îÇ  ‚îú‚îÄ Real-time requirements
‚îÇ  ‚îî‚îÄ Accuracy needs
‚îÇ
‚îú‚îÄ Data preparation
‚îÇ  ‚îú‚îÄ Image preprocessing (resize, normalize)
‚îÇ  ‚îú‚îÄ Data augmentation (flip, rotate, crop)
‚îÇ  ‚îú‚îÄ Handle class imbalance
‚îÇ  ‚îî‚îÄ Train/val/test split
‚îÇ
‚îú‚îÄ Model architecture
‚îÇ  ‚îú‚îÄ CNN basics (conv, pool, FC)
‚îÇ  ‚îú‚îÄ Pre-trained models (ResNet, VGG, EfficientNet)
‚îÇ  ‚îú‚îÄ Transfer learning
‚îÇ  ‚îú‚îÄ Object detection (YOLO, R-CNN)
‚îÇ  ‚îî‚îÄ Segmentation (U-Net, Mask R-CNN)
‚îÇ
‚îú‚îÄ Training
‚îÇ  ‚îú‚îÄ Loss function (cross-entropy, focal loss)
‚îÇ  ‚îú‚îÄ Optimizer and learning rate
‚îÇ  ‚îú‚îÄ Batch size
‚îÇ  ‚îî‚îÄ Monitor overfitting
‚îÇ
‚îî‚îÄ Evaluation
   ‚îú‚îÄ Accuracy, precision, recall
   ‚îú‚îÄ IoU for detection/segmentation
   ‚îú‚îÄ Confusion matrix
   ‚îî‚îÄ Visual inspection
""",
        "Recommendation Systems": """
Recommendation Systems
‚îú‚îÄ Understand the problem
‚îÇ  ‚îú‚îÄ User-item interactions
‚îÇ  ‚îú‚îÄ Cold start problem
‚îÇ  ‚îú‚îÄ Implicit vs explicit feedback
‚îÇ  ‚îî‚îÄ Business objectives
‚îÇ
‚îú‚îÄ Approach selection
‚îÇ  ‚îú‚îÄ Collaborative filtering (user-based, item-based)
‚îÇ  ‚îú‚îÄ Content-based filtering
‚îÇ  ‚îú‚îÄ Hybrid approaches
‚îÇ  ‚îú‚îÄ Matrix factorization
‚îÇ  ‚îî‚îÄ Deep learning (neural collaborative filtering)
‚îÇ
‚îú‚îÄ Feature engineering
‚îÇ  ‚îú‚îÄ User features (demographics, behavior)
‚îÇ  ‚îú‚îÄ Item features (attributes, popularity)
‚îÇ  ‚îú‚îÄ Context features (time, location)
‚îÇ  ‚îî‚îÄ Interaction features
‚îÇ
‚îú‚îÄ Model training
‚îÇ  ‚îú‚îÄ Handle sparsity
‚îÇ  ‚îú‚îÄ Negative sampling
‚îÇ  ‚îú‚îÄ Regularization
‚îÇ  ‚îî‚îÄ Optimize for ranking
‚îÇ
‚îî‚îÄ Evaluation
   ‚îú‚îÄ Offline: Precision@K, Recall@K, NDCG, MAP
   ‚îú‚îÄ Online: CTR, conversion rate
   ‚îú‚îÄ A/B testing
   ‚îî‚îÄ Diversity and serendipity
""",
        "Data Cleaning & Preprocessing": """
Data Cleaning & Preprocessing
‚îú‚îÄ Understand the data
‚îÇ  ‚îú‚îÄ Data types and schema
‚îÇ  ‚îú‚îÄ Data distributions
‚îÇ  ‚îú‚îÄ Summary statistics
‚îÇ  ‚îî‚îÄ Data quality issues
‚îÇ
‚îú‚îÄ Handle missing data
‚îÇ  ‚îú‚îÄ Identify patterns (MCAR, MAR, MNAR)
‚îÇ  ‚îú‚îÄ Remove rows/columns
‚îÇ  ‚îú‚îÄ Imputation (mean, median, mode, model-based)
‚îÇ  ‚îî‚îÄ Create missing indicator
‚îÇ
‚îú‚îÄ Handle outliers
‚îÇ  ‚îú‚îÄ Identify (IQR, z-score, visualization)
‚îÇ  ‚îú‚îÄ Investigate cause
‚îÇ  ‚îú‚îÄ Remove, cap, or transform
‚îÇ  ‚îî‚îÄ Robust methods
‚îÇ
‚îú‚îÄ Handle imbalanced data
‚îÇ  ‚îú‚îÄ Resampling (oversample minority, undersample majority)
‚îÇ  ‚îú‚îÄ SMOTE
‚îÇ  ‚îú‚îÄ Class weights
‚îÇ  ‚îî‚îÄ Anomaly detection approaches
‚îÇ
‚îî‚îÄ Transform and scale
   ‚îú‚îÄ Normalization (min-max)
   ‚îú‚îÄ Standardization (z-score)
   ‚îú‚îÄ Log/power transformations
   ‚îî‚îÄ Encoding (one-hot, label, target)
"""
    }
    
    return frameworks.get(category, "")

def main():
    """Generate comprehensive frameworks"""
    
    # Load data
    data_dir = os.path.join(os.path.dirname(__file__), '../data')
    with open(os.path.join(data_dir, 'questions_by_category.json'), 'r') as f:
        by_category = json.load(f)
    
    # Count totals
    total_questions = sum(len(qs) for qs in by_category.values())
    category_counts = [(cat, len(by_category[cat])) for cat in by_category.keys()]
    category_counts.sort(key=lambda x: x[1], reverse=True)
    
    print(f"üöÄ Generating comprehensive Data Scientist frameworks...")
    print(f"   Total questions: {total_questions}")
    print(f"   Categories: {len([c for c in category_counts if c[1] > 0])}")
    
    # Generate Question Bank (matching Data Analyst format)
    qb_md = """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                                ‚ïë
‚ïë           DATA SCIENTIST INTERVIEW PREPARATION FRAMEWORK                       ‚ïë
‚ïë           Mental Models & Complete Question Bank                               ‚ïë
‚ïë                                                                                ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

This framework provides mental models for approaching each type of data scientist
interview question. Focus on understanding the PATTERN and FRAMEWORK, not 
memorizing answers.

Total Questions: {} across {} categories


""".format(total_questions, len([c for c in category_counts if c[1] > 0]))
    
    # Add each category with framework
    for cat, count in category_counts:
        if count == 0:
            continue
        
        questions = by_category[cat]
        
        qb_md += "=" * 80 + "\n"
        qb_md += f"{cat.upper()}\n"
        qb_md += "=" * 80 + "\n\n"
        qb_md += f"üìä Total Questions: {count}\n\n"
        
        # Add "What they're really testing"
        testing_desc = {
            "Behavioral": "Can you demonstrate DS skills through past experiences using structured storytelling?",
            "Machine Learning - Supervised": "Can you build and evaluate supervised learning models effectively?",
            "Machine Learning - Unsupervised": "Can you discover patterns in unlabeled data?",
            "Machine Learning - Model Evaluation": "Can you properly evaluate and improve model performance?",
            "Machine Learning - Feature Engineering": "Can you create and select features that improve model performance?",
            "Statistics & Experimentation - A/B Testing": "Can you design, run, and analyze experiments rigorously?",
            "Statistics & Experimentation - Hypothesis Testing": "Can you test hypotheses using appropriate statistical methods?",
            "Statistics & Experimentation - Probability": "Can you apply probability theory to solve problems?",
            "Data Analysis - Root Cause": "Can you investigate and diagnose drops or changes in metrics?",
            "Data Analysis - Business Metrics": "Can you define and track the right business metrics?",
            "SQL": "Can you write efficient queries to extract insights from data?",
            "Coding": "Can you implement algorithms and solve coding problems?",
            "Model Deployment & Production": "Can you deploy and maintain models in production?",
            "Deep Learning": "Can you design and train deep neural networks?",
            "Time Series": "Can you forecast and analyze time-dependent data?",
            "NLP": "Can you process and analyze text data?",
            "Computer Vision": "Can you build models to understand images?",
            "Recommendation Systems": "Can you build systems that recommend relevant items?",
            "Data Cleaning & Preprocessing": "Can you prepare messy data for analysis and modeling?"
        }
        
        qb_md += f"üéØ What they're really testing:\n"
        qb_md += f"{testing_desc.get(cat, 'Your data science skills.')}\n\n"
        
        # Add framework
        framework = get_framework_for_category(cat)
        if framework:
            qb_md += "üó∫Ô∏è  Mental Model Framework:\n```\n"
            qb_md += framework.strip() + "\n```\n\n"
        
        # Add questions
        qb_md += f"üìù All {count} Questions:\n\n"
        for i, q in enumerate(questions, 1):
            qb_md += f"{i}. {q['question']}\n"
        
        qb_md += "\n"
    
    # Save Question Bank
    qb_path = os.path.join(os.path.dirname(__file__), '../Data_Scientist_Question_Bank.md')
    with open(qb_path, 'w') as f:
        f.write(qb_md)
    print(f"‚úÖ Generated Data_Scientist_Question_Bank.md")
    
    # Generate Interview Framework (high-level overview)
    fw_md = f"""# üß† Data Scientist Interview Mindmap (Systematic)

## üìö Resources

**GitHub Repo**: https://github.com/anix-lynch/Exponent

**Source**: https://www.tryexponent.com/questions?role=data-science

---

## üìä Question Distribution

```
"""
    
    for cat, count in category_counts:
        if count > 0:
            fw_md += f"{cat.ljust(50)} {count:>3} questions\n"
    
    fw_md += f"""```

**Total: {total_questions} questions across {len([c for c in category_counts if c[1] > 0])} categories**

---

## üéØ How to USE this in interviews

When a question comes:

1. **Name the category silently**
2. **Apply that category's framework**
3. Speak in **structured bullets**

---

## 0Ô∏è‚É£ Core Interview Meta-Structure (applies to EVERYTHING)

No matter the category, interviewers are testing:

- **Technical depth** - Do you understand the fundamentals?
- **Practical application** - Can you apply theory to real problems?
- **Communication** - Can you explain complex concepts clearly?
- **Business impact** - Do you connect models to business value?

So every answer should follow this shape:

```
Understand ‚Üí Plan ‚Üí Implement ‚Üí Evaluate ‚Üí Communicate impact
```

---

"""
    
    # Add key frameworks
    fw_md += "## Key Categories\n\n"
    for cat, count in category_counts[:10]:  # Top 10 categories
        if count > 0:
            fw_md += f"### {cat}\n\n"
            framework = get_framework_for_category(cat)
            if framework:
                fw_md += "```\n" + framework.strip() + "\n```\n\n"
            fw_md += "---\n\n"
    
    fw_md += """
## üí° Final Tips

### For All Data Scientist Interviews:

1. **Start with the problem** - Understand business context before jumping to solutions
2. **Show your thinking** - Walk through your approach step-by-step
3. **Quantify everything** - Use metrics to evaluate success
4. **Consider tradeoffs** - Accuracy vs speed, complexity vs interpretability
5. **Connect to business** - How does this model drive value?

### Common Mistakes to Avoid:

- ‚ùå Jumping to complex models without understanding the problem
- ‚ùå Ignoring data quality and preprocessing
- ‚ùå Overfitting to training data
- ‚ùå Not considering production constraints
- ‚ùå Forgetting to communicate business impact

---

**Check out the [Data_Scientist_Question_Bank.md](./Data_Scientist_Question_Bank.md) for all questions with detailed frameworks!**
"""
    
    # Save Interview Framework
    fw_path = os.path.join(os.path.dirname(__file__), '../INTERVIEW_FRAMEWORK.md')
    with open(fw_path, 'w') as f:
        f.write(fw_md)
    print(f"‚úÖ Generated INTERVIEW_FRAMEWORK.md")
    
    print("="*70)
    print("‚úÖ Data Scientist frameworks complete!")

if __name__ == "__main__":
    main()
