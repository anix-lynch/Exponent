# ğŸ§  ML Engineer Interview Mindmap (Systematic)

## ğŸ“š Resources

**GitHub Repo**: https://github.com/anix-lynch/Exponent

**Source**: https://www.tryexponent.com/questions?page=1&role=ml-engineer

---

## ğŸ“Š Question Distribution

```
Machine Learning........................  103 questions
Data Structures & Algorithms............   39 questions
System Design...........................   20 questions
Behavioral..............................   20 questions
Computer Vision.........................   11 questions
ML System Design........................   11 questions
Model Evaluation........................   10 questions
Coding..................................    9 questions
Deep Learning...........................    8 questions
Model Deployment........................    8 questions
LLMs....................................    7 questions
Reinforcement Learning..................    6 questions
Generative AI...........................    6 questions
Natural Language Processing.............    5 questions
SQL.....................................    4 questions
Statistics & Probability................    3 questions
MLOps...................................    2 questions
Optimization............................    1 question
```

**Total: 250 questions across 18 categories**

---

## ğŸ”Ÿ How to USE this in interviews

When a question comes:

1. **Name the category silently**
2. **Apply that category's framework**
3. **Speak in structured bullets**

---

## 0ï¸âƒ£ Core ML Engineering Meta-Structure

Every ML interview tests:

- **ML fundamentals** (algorithms, evaluation, optimization)
- **System thinking** (end-to-end ML systems)
- **Coding ability** (implement algorithms, process data)
- **Production mindset** (deployment, monitoring, scaling)

So every answer should follow:

```
Clarify â†’ Design â†’ Implement â†’ Evaluate â†’ Deploy â†’ Monitor
```

---

## 1ï¸âƒ£ Machine Learning (CRITICAL - 103 questions)

**What they're really testing:**
Do you understand ML fundamentals and when to apply different algorithms?

**Mindmap**

```
Machine Learning
â”œâ”€ 1. Problem Understanding
â”‚  â”œâ”€ Classification vs Regression vs Clustering
â”‚  â”œâ”€ Supervised vs Unsupervised vs Semi-supervised
â”‚  â”œâ”€ Available data (labeled/unlabeled)
â”‚  â””â”€ Success metrics
â”‚
â”œâ”€ 2. Algorithm Selection
â”‚  â”œâ”€ Linear models (LR, Lasso, Ridge)
â”‚  â”œâ”€ Tree-based (Decision Tree, Random Forest, XGBoost)
â”‚  â”œâ”€ SVM (Support Vector Machine)
â”‚  â”œâ”€ Neural Networks
â”‚  â””â”€ Ensemble methods
â”‚
â”œâ”€ 3. Feature Engineering
â”‚  â”œâ”€ Feature selection (filter, wrapper, embedded)
â”‚  â”œâ”€ Feature extraction (PCA, t-SNE)
â”‚  â”œâ”€ Encoding (one-hot, label, target)
â”‚  â”œâ”€ Scaling (standardization, normalization)
â”‚  â””â”€ Handling missing values
â”‚
â”œâ”€ 4. Training
â”‚  â”œâ”€ Train/val/test split (70/15/15 or 80/10/10)
â”‚  â”œâ”€ Cross-validation (k-fold)
â”‚  â”œâ”€ Hyperparameter tuning (grid search, random search)
â”‚  â””â”€ Regularization (L1, L2, dropout)
â”‚
â”œâ”€ 5. Evaluation
â”‚  â”œâ”€ Classification metrics (accuracy, precision, recall, F1, AUC-ROC)
â”‚  â”œâ”€ Regression metrics (MSE, RMSE, MAE, RÂ²)
â”‚  â”œâ”€ Clustering metrics (silhouette, Davies-Bouldin)
â”‚  â””â”€ Confusion matrix analysis
â”‚
â””â”€ 6. Model Improvement
   â”œâ”€ Address overfitting (more data, regularization, simpler model)
   â”œâ”€ Address underfitting (more features, complex model)
   â”œâ”€ Feature engineering iteration
   â””â”€ Ensemble methods
```

ğŸ“Œ **Always start with**: What type of problem? What data? What metric?

---

## 2ï¸âƒ£ Data Structures & Algorithms (HIGH - 39 questions)

**What they're really testing:**
Can you write efficient code to solve problems?

**Mindmap**

```
DSA for ML Engineers
â”œâ”€ 1. Problem Understanding
â”‚  â”œâ”€ Input format
â”‚  â”œâ”€ Output format
â”‚  â”œâ”€ Constraints
â”‚  â””â”€ Edge cases
â”‚
â”œâ”€ 2. Data Structure Choice
â”‚  â”œâ”€ Array/List (sequential access)
â”‚  â”œâ”€ Hash Map/Set (O(1) lookup)
â”‚  â”œâ”€ Tree (hierarchical data)
â”‚  â”œâ”€ Graph (relationships)
â”‚  â”œâ”€ Heap (priority queue)
â”‚  â””â”€ Stack/Queue (LIFO/FIFO)
â”‚
â”œâ”€ 3. Algorithm Patterns
â”‚  â”œâ”€ Two pointers
â”‚  â”œâ”€ Sliding window
â”‚  â”œâ”€ BFS/DFS
â”‚  â”œâ”€ Dynamic programming
â”‚  â”œâ”€ Binary search
â”‚  â””â”€ Divide and conquer
â”‚
â”œâ”€ 4. Implementation
â”‚  â”œâ”€ Write clean code
â”‚  â”œâ”€ Handle edge cases
â”‚  â”œâ”€ Test with examples
â”‚  â””â”€ Optimize
â”‚
â””â”€ 5. Complexity Analysis
   â”œâ”€ Time complexity: O(?)
   â”œâ”€ Space complexity: O(?)
   â””â”€ Can we do better?
```

ğŸ“Œ **Think in terms of**: Time/space complexity, common patterns, edge cases

---

## 3ï¸âƒ£ ML System Design (HIGH - 11 questions)

**What they're really testing:**
Can you design end-to-end ML systems at scale?

**Mindmap**

```
ML System Design
â”œâ”€ 1. Requirements (5 min)
â”‚  â”œâ”€ Problem definition
â”‚  â”‚  â”œâ”€ What are we predicting?
â”‚  â”‚  â”œâ”€ What's the business goal?
â”‚  â”‚  â””â”€ Success metrics
â”‚  â”œâ”€ Scale
â”‚  â”‚  â”œâ”€ QPS (queries per second)
â”‚  â”‚  â”œâ”€ Number of users
â”‚  â”‚  â””â”€ Data volume
â”‚  â”œâ”€ Latency requirements
â”‚  â”‚  â”œâ”€ Real-time (< 100ms)
â”‚  â”‚  â”œâ”€ Near real-time (< 1s)
â”‚  â”‚  â””â”€ Batch (minutes/hours)
â”‚  â””â”€ Accuracy requirements
â”‚
â”œâ”€ 2. High-Level Design (10 min)
â”‚  â”œâ”€ Data collection
â”‚  â”‚  â”œâ”€ User events
â”‚  â”‚  â”œâ”€ Logs
â”‚  â”‚  â””â”€ External APIs
â”‚  â”œâ”€ Feature engineering
â”‚  â”‚  â”œâ”€ Online features (real-time)
â”‚  â”‚  â””â”€ Offline features (batch)
â”‚  â”œâ”€ Model training
â”‚  â”‚  â”œâ”€ Training pipeline
â”‚  â”‚  â”œâ”€ Model selection
â”‚  â”‚  â””â”€ Hyperparameter tuning
â”‚  â”œâ”€ Model serving
â”‚  â”‚  â”œâ”€ Prediction API
â”‚  â”‚  â”œâ”€ Caching
â”‚  â”‚  â””â”€ Load balancing
â”‚  â””â”€ Monitoring & feedback
â”‚     â”œâ”€ Model performance
â”‚     â”œâ”€ Data drift
â”‚     â””â”€ A/B testing
â”‚
â”œâ”€ 3. Deep Dive (20 min)
â”‚  â”œâ”€ Model choice & architecture
â”‚  â”‚  â”œâ”€ Why this model?
â”‚  â”‚  â”œâ”€ Alternatives considered
â”‚  â”‚  â””â”€ Trade-offs
â”‚  â”œâ”€ Training pipeline
â”‚  â”‚  â”œâ”€ Data preprocessing
â”‚  â”‚  â”œâ”€ Feature store
â”‚  â”‚  â”œâ”€ Training frequency
â”‚  â”‚  â””â”€ Experiment tracking
â”‚  â”œâ”€ Inference optimization
â”‚  â”‚  â”œâ”€ Model compression
â”‚  â”‚  â”œâ”€ Quantization
â”‚  â”‚  â”œâ”€ Batching
â”‚  â”‚  â””â”€ GPU utilization
â”‚  â”œâ”€ A/B testing
â”‚  â”‚  â”œâ”€ Control vs treatment
â”‚  â”‚  â”œâ”€ Metrics to track
â”‚  â”‚  â””â”€ Statistical significance
â”‚  â””â”€ Feedback loop
â”‚     â”œâ”€ Collect predictions
â”‚     â”œâ”€ Collect ground truth
â”‚     â””â”€ Retrain model
â”‚
â””â”€ 4. Trade-offs
   â”œâ”€ Accuracy vs Latency
   â”œâ”€ Complexity vs Interpretability
   â”œâ”€ Cost vs Performance
   â””â”€ Online vs Offline learning
```

ğŸ“Œ **Always discuss**: Scale, latency, accuracy, trade-offs

---

## 4ï¸âƒ£ System Design (20 questions)

**What they're really testing:**
Can you design scalable distributed systems?

**Mindmap**

```
System Design
â”œâ”€ 1. Requirements (5 min)
â”‚  â”œâ”€ Functional requirements
â”‚  â”‚  â”œâ”€ What features?
â”‚  â”‚  â””â”€ What APIs?
â”‚  â”œâ”€ Non-functional requirements
â”‚  â”‚  â”œâ”€ Scale (QPS, users, data)
â”‚  â”‚  â”œâ”€ Latency (ms, seconds)
â”‚  â”‚  â”œâ”€ Availability (99.9%?)
â”‚  â”‚  â””â”€ Consistency
â”‚  â””â”€ Constraints
â”‚
â”œâ”€ 2. High-Level Design (10 min)
â”‚  â”œâ”€ Draw architecture
â”‚  â”œâ”€ Main components
â”‚  â”œâ”€ Data flow
â”‚  â””â”€ APIs
â”‚
â”œâ”€ 3. Deep Dive (20 min)
â”‚  â”œâ”€ Database design
â”‚  â”‚  â”œâ”€ SQL vs NoSQL
â”‚  â”‚  â”œâ”€ Schema design
â”‚  â”‚  â””â”€ Indexing
â”‚  â”œâ”€ Caching
â”‚  â”‚  â”œâ”€ Redis, Memcached
â”‚  â”‚  â”œâ”€ Cache invalidation
â”‚  â”‚  â””â”€ TTL strategy
â”‚  â”œâ”€ Load balancing
â”‚  â”‚  â”œâ”€ Round robin
â”‚  â”‚  â”œâ”€ Least connections
â”‚  â”‚  â””â”€ Consistent hashing
â”‚  â””â”€ Scaling
â”‚     â”œâ”€ Horizontal scaling
â”‚     â”œâ”€ Vertical scaling
â”‚     â”œâ”€ Sharding
â”‚     â””â”€ Replication
â”‚
â””â”€ 4. Trade-offs
   â”œâ”€ Consistency vs Availability (CAP theorem)
   â”œâ”€ Latency vs Throughput
   â””â”€ Cost vs Performance
```

ğŸ“Œ **Draw diagrams**: Visual communication is key

---

## 5ï¸âƒ£ Behavioral (20 questions)

**What they're really testing:**
Can you work effectively in a team?

**Mindmap (STAR Method)**

```
Behavioral
â”œâ”€ Situation
â”‚  â”œâ”€ Context
â”‚  â”œâ”€ Challenge
â”‚  â””â”€ Stakeholders
â”‚
â”œâ”€ Task
â”‚  â”œâ”€ Your role
â”‚  â”œâ”€ Goal
â”‚  â””â”€ Constraints
â”‚
â”œâ”€ Action
â”‚  â”œâ”€ What YOU did
â”‚  â”œâ”€ Technical decisions
â”‚  â”œâ”€ Trade-offs considered
â”‚  â””â”€ Collaboration
â”‚
â””â”€ Result
   â”œâ”€ Quantifiable impact
   â”œâ”€ What you learned
   â””â”€ What you'd do differently
```

ğŸ“Œ **Common themes**: Project ownership, technical challenges, collaboration, learning from failure

---

## 6ï¸âƒ£ Computer Vision (11 questions)

**What they're really testing:**
Can you build and deploy computer vision models?

**Mindmap**

```
Computer Vision
â”œâ”€ 1. Problem Type
â”‚  â”œâ”€ Image classification
â”‚  â”œâ”€ Object detection (YOLO, R-CNN)
â”‚  â”œâ”€ Semantic segmentation
â”‚  â”œâ”€ Instance segmentation
â”‚  â””â”€ Image generation
â”‚
â”œâ”€ 2. Architecture
â”‚  â”œâ”€ CNN (Convolutional Neural Network)
â”‚  â”‚  â”œâ”€ Conv layers
â”‚  â”‚  â”œâ”€ Pooling layers
â”‚  â”‚  â””â”€ Fully connected layers
â”‚  â”œâ”€ Pre-trained models
â”‚  â”‚  â”œâ”€ ResNet
â”‚  â”‚  â”œâ”€ VGG
â”‚  â”‚  â”œâ”€ EfficientNet
â”‚  â”‚  â””â”€ Vision Transformer (ViT)
â”‚  â””â”€ Transfer learning
â”‚
â”œâ”€ 3. Data Preprocessing
â”‚  â”œâ”€ Normalization
â”‚  â”œâ”€ Augmentation (flip, rotate, crop)
â”‚  â”œâ”€ Resizing
â”‚  â””â”€ Color space conversion
â”‚
â”œâ”€ 4. Training
â”‚  â”œâ”€ Loss function (cross-entropy, focal loss)
â”‚  â”œâ”€ Optimizer (Adam, SGD)
â”‚  â”œâ”€ Learning rate schedule
â”‚  â””â”€ Data augmentation
â”‚
â””â”€ 5. Evaluation
   â”œâ”€ Accuracy
   â”œâ”€ IoU (Intersection over Union)
   â”œâ”€ mAP (mean Average Precision)
   â””â”€ Confusion matrix
```

ğŸ“Œ **Transfer learning first**: Start with pre-trained models, fine-tune on your data

---

## 7ï¸âƒ£ Model Evaluation (10 questions)

**What they're really testing:**
Do you know how to evaluate and improve ML models?

**Mindmap**

```
Model Evaluation
â”œâ”€ 1. Classification Metrics
â”‚  â”œâ”€ Accuracy (when balanced)
â”‚  â”œâ”€ Precision (minimize false positives)
â”‚  â”œâ”€ Recall (minimize false negatives)
â”‚  â”œâ”€ F1 score (harmonic mean)
â”‚  â”œâ”€ AUC-ROC (threshold-independent)
â”‚  â””â”€ Confusion matrix
â”‚
â”œâ”€ 2. Regression Metrics
â”‚  â”œâ”€ MSE (Mean Squared Error)
â”‚  â”œâ”€ RMSE (Root MSE)
â”‚  â”œâ”€ MAE (Mean Absolute Error)
â”‚  â””â”€ RÂ² (coefficient of determination)
â”‚
â”œâ”€ 3. Overfitting vs Underfitting
â”‚  â”œâ”€ Overfitting
â”‚  â”‚  â”œâ”€ High train accuracy, low val accuracy
â”‚  â”‚  â”œâ”€ Solutions: regularization, more data, simpler model
â”‚  â”‚  â””â”€ Dropout, early stopping
â”‚  â””â”€ Underfitting
â”‚     â”œâ”€ Low train and val accuracy
â”‚     â”œâ”€ Solutions: more features, complex model
â”‚     â””â”€ Feature engineering
â”‚
â”œâ”€ 4. Bias-Variance Trade-off
â”‚  â”œâ”€ High bias = underfitting
â”‚  â”œâ”€ High variance = overfitting
â”‚  â””â”€ Balance through regularization
â”‚
â””â”€ 5. Cross-Validation
   â”œâ”€ K-fold CV
   â”œâ”€ Stratified K-fold
   â””â”€ Time series CV
```

ğŸ“Œ **Choose metric based on problem**: Imbalanced data? Use F1/AUC. Outliers matter? Use MAE.

---

## 8ï¸âƒ£ Coding (9 questions)

**What they're really testing:**
Can you implement ML algorithms from scratch?

**Mindmap**

```
Coding
â”œâ”€ 1. Understand requirements
â”‚  â”œâ”€ Input format
â”‚  â”œâ”€ Output format
â”‚  â”œâ”€ Constraints
â”‚  â””â”€ Edge cases
â”‚
â”œâ”€ 2. Design approach
â”‚  â”œâ”€ Algorithm choice
â”‚  â”œâ”€ Data structures
â”‚  â””â”€ Pseudocode
â”‚
â”œâ”€ 3. Implement
â”‚  â”œâ”€ Clean code
â”‚  â”œâ”€ Meaningful names
â”‚  â”œâ”€ Comments
â”‚  â””â”€ Error handling
â”‚
â”œâ”€ 4. Test
â”‚  â”œâ”€ Normal cases
â”‚  â”œâ”€ Edge cases
â”‚  â””â”€ Performance
â”‚
â””â”€ 5. Optimize
   â”œâ”€ Time complexity
   â”œâ”€ Space complexity
   â””â”€ Code readability
```

ğŸ“Œ **Common tasks**: Implement gradient descent, backpropagation, k-means, decision tree

---

## 9ï¸âƒ£ Deep Learning (8 questions)

**What they're really testing:**
Can you design and train neural networks effectively?

**Mindmap**

```
Deep Learning
â”œâ”€ 1. Architecture Design
â”‚  â”œâ”€ Input layer (match data dimensions)
â”‚  â”œâ”€ Hidden layers
â”‚  â”‚  â”œâ”€ Number of layers (depth)
â”‚  â”‚  â”œâ”€ Number of neurons (width)
â”‚  â”‚  â””â”€ Activation functions (ReLU, tanh, sigmoid)
â”‚  â””â”€ Output layer (match task)
â”‚
â”œâ”€ 2. Training
â”‚  â”œâ”€ Loss function
â”‚  â”‚  â”œâ”€ Classification: Cross-entropy
â”‚  â”‚  â”œâ”€ Regression: MSE
â”‚  â”‚  â””â”€ Custom losses
â”‚  â”œâ”€ Optimizer
â”‚  â”‚  â”œâ”€ SGD (simple, stable)
â”‚  â”‚  â”œâ”€ Adam (adaptive, fast)
â”‚  â”‚  â””â”€ RMSprop
â”‚  â”œâ”€ Learning rate
â”‚  â”‚  â”œâ”€ Initial value
â”‚  â”‚  â”œâ”€ Schedule (decay, warmup)
â”‚  â”‚  â””â”€ Learning rate finder
â”‚  â””â”€ Batch size
â”‚     â”œâ”€ Small batch: noisy gradients, regularization
â”‚     â””â”€ Large batch: stable, faster training
â”‚
â”œâ”€ 3. Regularization
â”‚  â”œâ”€ Dropout (randomly drop neurons)
â”‚  â”œâ”€ Batch normalization (normalize activations)
â”‚  â”œâ”€ L1/L2 regularization (weight penalty)
â”‚  â”œâ”€ Early stopping (stop when val loss increases)
â”‚  â””â”€ Data augmentation
â”‚
â”œâ”€ 4. Optimization Techniques
â”‚  â”œâ”€ Gradient descent variants
â”‚  â”œâ”€ Momentum
â”‚  â”œâ”€ Gradient clipping
â”‚  â””â”€ Batch normalization
â”‚
â””â”€ 5. Evaluation
   â”œâ”€ Training loss
   â”œâ”€ Validation loss
   â”œâ”€ Test accuracy
   â””â”€ Learning curves
```

ğŸ“Œ **Start simple**: Shallow network first, then add complexity if needed

---

## ğŸ”Ÿ Model Deployment (8 questions)

**What they're really testing:**
Can you deploy and serve ML models in production?

**Mindmap**

```
Model Deployment
â”œâ”€ 1. Serving Infrastructure
â”‚  â”œâ”€ REST API (Flask, FastAPI)
â”‚  â”œâ”€ gRPC (low latency)
â”‚  â”œâ”€ Batch prediction
â”‚  â”œâ”€ Real-time prediction
â”‚  â””â”€ Load balancing
â”‚
â”œâ”€ 2. Model Optimization
â”‚  â”œâ”€ Model compression
â”‚  â”‚  â”œâ”€ Pruning (remove weights)
â”‚  â”‚  â”œâ”€ Quantization (reduce precision)
â”‚  â”‚  â””â”€ Knowledge distillation
â”‚  â”œâ”€ Inference optimization
â”‚  â”‚  â”œâ”€ Batching requests
â”‚  â”‚  â”œâ”€ Caching predictions
â”‚  â”‚  â””â”€ GPU utilization
â”‚  â””â”€ Model formats
â”‚     â”œâ”€ ONNX
â”‚     â”œâ”€ TensorFlow Lite
â”‚     â””â”€ TorchScript
â”‚
â”œâ”€ 3. Monitoring
â”‚  â”œâ”€ Latency (p50, p95, p99)
â”‚  â”œâ”€ Throughput (QPS)
â”‚  â”œâ”€ Error rate
â”‚  â”œâ”€ Model drift
â”‚  â”‚  â”œâ”€ Data drift (input distribution changes)
â”‚  â”‚  â””â”€ Concept drift (relationship changes)
â”‚  â””â”€ Resource utilization (CPU, GPU, memory)
â”‚
â”œâ”€ 4. Scaling
â”‚  â”œâ”€ Horizontal scaling (add more servers)
â”‚  â”œâ”€ Vertical scaling (bigger servers)
â”‚  â”œâ”€ Auto-scaling
â”‚  â””â”€ GPU sharing
â”‚
â””â”€ 5. A/B Testing
   â”œâ”€ Control vs treatment
   â”œâ”€ Metrics to track
   â”œâ”€ Statistical significance
   â””â”€ Gradual rollout
```

ğŸ“Œ **Production = monitoring + optimization + reliability**

---

## 1ï¸âƒ£1ï¸âƒ£ LLMs (7 questions)

**What they're really testing:**
Do you understand how large language models work?

**Mindmap**

```
LLMs (Large Language Models)
â”œâ”€ 1. Architecture
â”‚  â”œâ”€ Transformer
â”‚  â”‚  â”œâ”€ Self-attention mechanism
â”‚  â”‚  â”œâ”€ Multi-head attention
â”‚  â”‚  â””â”€ Positional encoding
â”‚  â”œâ”€ Context window
â”‚  â”‚  â”œâ”€ Token limit (e.g., 4K, 8K, 128K)
â”‚  â”‚  â”œâ”€ Lost in the middle problem
â”‚  â”‚  â””â”€ Context management
â”‚  â””â”€ Tokenization
â”‚     â”œâ”€ BPE (Byte Pair Encoding)
â”‚     â”œâ”€ WordPiece
â”‚     â””â”€ SentencePiece
â”‚
â”œâ”€ 2. Training
â”‚  â”œâ”€ Pre-training (unsupervised)
â”‚  â”‚  â”œâ”€ Next token prediction
â”‚  â”‚  â”œâ”€ Masked language modeling
â”‚  â”‚  â””â”€ Large corpus
â”‚  â”œâ”€ Fine-tuning (supervised)
â”‚  â”‚  â”œâ”€ Task-specific data
â”‚  â”‚  â”œâ”€ Instruction tuning
â”‚  â”‚  â””â”€ Few-shot learning
â”‚  â””â”€ RLHF (Reinforcement Learning from Human Feedback)
â”‚     â”œâ”€ Reward model
â”‚     â”œâ”€ PPO (Proximal Policy Optimization)
â”‚     â””â”€ Human preferences
â”‚
â”œâ”€ 3. Inference
â”‚  â”œâ”€ Sampling strategies
â”‚  â”‚  â”œâ”€ Greedy (deterministic)
â”‚  â”‚  â”œâ”€ Temperature (randomness)
â”‚  â”‚  â”œâ”€ Top-k sampling
â”‚  â”‚  â””â”€ Top-p (nucleus) sampling
â”‚  â”œâ”€ Prompt engineering
â”‚  â”‚  â”œâ”€ Zero-shot
â”‚  â”‚  â”œâ”€ Few-shot
â”‚  â”‚  â””â”€ Chain-of-thought
â”‚  â””â”€ Context management
â”‚     â”œâ”€ Sliding window
â”‚     â”œâ”€ Summarization
â”‚     â””â”€ Retrieval augmentation (RAG)
â”‚
â””â”€ 4. Evaluation
   â”œâ”€ Perplexity
   â”œâ”€ BLEU, ROUGE (text generation)
   â”œâ”€ Human evaluation
   â””â”€ Task-specific metrics
```

ğŸ“Œ **Context window is key**: Longer context = more information but higher cost and latency

---

## 1ï¸âƒ£2ï¸âƒ£ Reinforcement Learning (6 questions)

**What they're really testing:**
Do you understand RL concepts and algorithms?

**Mindmap**

```
Reinforcement Learning
â”œâ”€ 1. Core Concepts
â”‚  â”œâ”€ Agent (learner)
â”‚  â”œâ”€ Environment (world)
â”‚  â”œâ”€ State (current situation)
â”‚  â”œâ”€ Action (what agent can do)
â”‚  â”œâ”€ Reward (feedback)
â”‚  â””â”€ Policy (strategy)
â”‚
â”œâ”€ 2. MDP (Markov Decision Process)
â”‚  â”œâ”€ States S
â”‚  â”œâ”€ Actions A
â”‚  â”œâ”€ Transition probabilities P
â”‚  â”œâ”€ Rewards R
â”‚  â””â”€ Discount factor Î³
â”‚
â”œâ”€ 3. Algorithms
â”‚  â”œâ”€ Value-based
â”‚  â”‚  â”œâ”€ Q-Learning
â”‚  â”‚  â”œâ”€ DQN (Deep Q-Network)
â”‚  â”‚  â””â”€ Double DQN
â”‚  â”œâ”€ Policy-based
â”‚  â”‚  â”œâ”€ Policy Gradient
â”‚  â”‚  â”œâ”€ REINFORCE
â”‚  â”‚  â””â”€ Actor-Critic
â”‚  â””â”€ Model-based
â”‚     â”œâ”€ Planning
â”‚     â””â”€ Monte Carlo Tree Search
â”‚
â”œâ”€ 4. Exploration vs Exploitation
â”‚  â”œâ”€ Îµ-greedy
â”‚  â”œâ”€ Softmax
â”‚  â””â”€ UCB (Upper Confidence Bound)
â”‚
â””â”€ 5. Applications
   â”œâ”€ Game playing
   â”œâ”€ Robotics
   â”œâ”€ Recommendation systems
   â””â”€ Resource allocation
```

ğŸ“Œ **Key trade-off**: Exploration (try new things) vs Exploitation (use what works)

---

## 1ï¸âƒ£3ï¸âƒ£ Generative AI (6 questions)

**What they're really testing:**
Can you work with generative models?

**Mindmap**

```
Generative AI
â”œâ”€ 1. Model Types
â”‚  â”œâ”€ GANs (Generative Adversarial Networks)
â”‚  â”‚  â”œâ”€ Generator (creates fake data)
â”‚  â”‚  â”œâ”€ Discriminator (detects fake)
â”‚  â”‚  â””â”€ Adversarial training
â”‚  â”œâ”€ VAE (Variational Autoencoder)
â”‚  â”‚  â”œâ”€ Encoder (compress to latent)
â”‚  â”‚  â”œâ”€ Decoder (reconstruct)
â”‚  â”‚  â””â”€ Probabilistic
â”‚  â”œâ”€ Diffusion Models
â”‚  â”‚  â”œâ”€ Forward process (add noise)
â”‚  â”‚  â”œâ”€ Reverse process (denoise)
â”‚  â”‚  â””â”€ Stable Diffusion, DALL-E
â”‚  â””â”€ Transformers (GPT, etc.)
â”‚
â”œâ”€ 2. Training
â”‚  â”œâ”€ Loss functions
â”‚  â”œâ”€ Mode collapse (GANs)
â”‚  â”œâ”€ Convergence issues
â”‚  â””â”€ Regularization
â”‚
â”œâ”€ 3. Evaluation
â”‚  â”œâ”€ Inception Score
â”‚  â”œâ”€ FID (FrÃ©chet Inception Distance)
â”‚  â”œâ”€ Human evaluation
â”‚  â””â”€ Diversity metrics
â”‚
â””â”€ 4. Applications
   â”œâ”€ Image generation
   â”œâ”€ Text generation
   â”œâ”€ Audio synthesis
   â””â”€ Video generation
```

ğŸ“Œ **Diffusion models > GANs** for image generation (more stable training)

---

## 1ï¸âƒ£4ï¸âƒ£ Natural Language Processing (5 questions)

**What they're really testing:**
Do you understand NLP techniques?

**Mindmap**

```
Natural Language Processing
â”œâ”€ 1. Text Preprocessing
â”‚  â”œâ”€ Tokenization
â”‚  â”œâ”€ Lowercasing
â”‚  â”œâ”€ Stop word removal
â”‚  â”œâ”€ Stemming/Lemmatization
â”‚  â””â”€ Handling special characters
â”‚
â”œâ”€ 2. Text Representation
â”‚  â”œâ”€ Bag of Words
â”‚  â”œâ”€ TF-IDF
â”‚  â”œâ”€ Word embeddings (Word2Vec, GloVe)
â”‚  â”œâ”€ Contextual embeddings (BERT, GPT)
â”‚  â””â”€ Sentence embeddings
â”‚
â”œâ”€ 3. Common Tasks
â”‚  â”œâ”€ Classification (sentiment, topic)
â”‚  â”œâ”€ Named Entity Recognition (NER)
â”‚  â”œâ”€ Question Answering
â”‚  â”œâ”€ Machine Translation
â”‚  â””â”€ Text Summarization
â”‚
â”œâ”€ 4. Models
â”‚  â”œâ”€ Traditional (Naive Bayes, SVM)
â”‚  â”œâ”€ RNN/LSTM
â”‚  â”œâ”€ Transformer (BERT, GPT)
â”‚  â””â”€ Fine-tuning pre-trained models
â”‚
â””â”€ 5. Evaluation
   â”œâ”€ Accuracy, F1
   â”œâ”€ BLEU (translation)
   â”œâ”€ ROUGE (summarization)
   â””â”€ Perplexity (language modeling)
```

ğŸ“Œ **Start with pre-trained models**: BERT for understanding, GPT for generation

---

## 1ï¸âƒ£5ï¸âƒ£ SQL (4 questions)

**What they're really testing:**
Can you query and manipulate data?

**Mindmap**

```
SQL
â”œâ”€ SELECT / WHERE
â”œâ”€ JOIN (INNER, LEFT, RIGHT)
â”œâ”€ GROUP BY / HAVING
â”œâ”€ Aggregations (COUNT, SUM, AVG)
â”œâ”€ Window functions
â””â”€ CTEs (WITH clause)
```

ğŸ“Œ **For ML**: Use SQL to prepare training data

---

## 1ï¸âƒ£6ï¸âƒ£ Statistics & Probability (3 questions)

**What they're really testing:**
Do you have strong statistical foundations?

**Mindmap**

```
Statistics & Probability
â”œâ”€ Distributions (Normal, Binomial, Poisson)
â”œâ”€ Hypothesis testing
â”œâ”€ P-value, confidence intervals
â”œâ”€ Bayesian vs Frequentist
â””â”€ Central Limit Theorem
```

ğŸ“Œ **Foundation for ML**: Understanding uncertainty and inference

---

## 1ï¸âƒ£7ï¸âƒ£ MLOps (2 questions)

**What they're really testing:**
Do you understand ML operations?

**Mindmap**

```
MLOps
â”œâ”€ Experiment tracking (MLflow, Weights & Biases)
â”œâ”€ Model versioning
â”œâ”€ CI/CD for ML
â”œâ”€ Feature store
â””â”€ Model registry
```

ğŸ“Œ **Production ML = MLOps**

---

## ğŸ¯ Study Strategy

### Week 1-2: ML Fundamentals (103 questions)
- Review algorithms, evaluation metrics
- Practice on Kaggle datasets
- Implement algorithms from scratch

### Week 3-4: DSA (39 questions)
- Daily LeetCode (Medium level)
- Focus on: Arrays, Trees, DP
- ML-specific problems (matrix operations)

### Week 5-6: ML System Design (11 questions)
- Practice designing: Recommendation, Search, Ranking systems
- Study: Feature stores, model serving, A/B testing
- Draw architecture diagrams

### Ongoing:
- Behavioral (STAR method)
- Deep Learning, Computer Vision, NLP
- Stay updated on LLMs and Generative AI

---

**See [`ML_Engineer_Question_Bank.md`](./ML_Engineer_Question_Bank.md) for all questions with frameworks.**
